{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lyrics_generation.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"FDg9T6d9N0Vy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606286956803,"user_tz":-420,"elapsed":29263,"user":{"displayName":"Tuấn Tô Huỳnh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXJTUPH4_1EZB9BBjAzQVNJqit9A6jglPVLllHoQ=s64","userId":"04616076029360035217"}},"outputId":"5aaa035b-1212-4396-96d1-d3f72df82f16"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7nLNUX-1NoPJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606286983392,"user_tz":-420,"elapsed":1928,"user":{"displayName":"Tuấn Tô Huỳnh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXJTUPH4_1EZB9BBjAzQVNJqit9A6jglPVLllHoQ=s64","userId":"04616076029360035217"}},"outputId":"19b5c7bc-3423-43f9-ac89-953db313dda0"},"source":["cd /content/drive/MyDrive/tom_chang"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/tom_chang\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9U7yEYI8lePf","executionInfo":{"status":"ok","timestamp":1606287259014,"user_tz":-420,"elapsed":792,"user":{"displayName":"Tuấn Tô Huỳnh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXJTUPH4_1EZB9BBjAzQVNJqit9A6jglPVLllHoQ=s64","userId":"04616076029360035217"}},"outputId":"0856e849-54c5-4c00-fcc5-5b199d9b7d59"},"source":["ls"],"execution_count":7,"outputs":[{"output_type":"stream","text":["chord_dictionary_name_id.json   LICENSE           \u001b[0m\u001b[01;34m__pycache__\u001b[0m/\n","Config.py                       lyric_ballad.csv  README.md\n","convert_string_to_int_chord.py  \u001b[01;34mlyrics\u001b[0m/           requirements_test.txt\n","final.csv                       \u001b[01;34mmodel\u001b[0m/            requirements.txt\n","final_old.csv                   Model.py          tach_tu.py\n","generate.py                     Model.pyc         train.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pUX5WwfhKmYS","executionInfo":{"status":"ok","timestamp":1606287581865,"user_tz":-420,"elapsed":101420,"user":{"displayName":"Tuấn Tô Huỳnh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXJTUPH4_1EZB9BBjAzQVNJqit9A6jglPVLllHoQ=s64","userId":"04616076029360035217"}},"outputId":"9b7f10ff-425f-4425-c31d-31b6c81f9af3"},"source":["!python train_converted.py"],"execution_count":11,"outputs":[{"output_type":"stream","text":["2020-11-25 13:57:59.134904: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2020-11-25 13:57:59.134946: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n","11086\tTình là thế\n","11106\tGiấc mơ tình yêu\n","^C\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"cBgjLzVpLrNu","executionInfo":{"status":"ok","timestamp":1606287392899,"user_tz":-420,"elapsed":68883,"user":{"displayName":"Tuấn Tô Huỳnh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXJTUPH4_1EZB9BBjAzQVNJqit9A6jglPVLllHoQ=s64","userId":"04616076029360035217"}},"outputId":"abc34eaf-c656-49de-8e69-00644a07c65e"},"source":["!pip install tf-nightly\n","!tf_upgrade_v2 --infile ./train.py --outfile ./train_converted.py"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Collecting tf-nightly\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/70/08072e4afe878e2183b44744f927b9d2640394240035531f042ce63bda9b/tf_nightly-2.5.0.dev20201125-cp36-cp36m-manylinux2010_x86_64.whl (397.5MB)\n","\u001b[K     |████████████████████████████████| 397.5MB 42kB/s \n","\u001b[?25hRequirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.6.3)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.35.1)\n","Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.3.3)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.2)\n","Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.3.0)\n","Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.7.4.3)\n","Collecting tb-nightly~=2.5.0.a\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/67/be02f453a7ffd2c62cbbb45f06b82a52437972ebc3eaac59065bdaf2de7b/tb_nightly-2.5.0a20201125-py3-none-any.whl (11.9MB)\n","\u001b[K     |████████████████████████████████| 11.9MB 268kB/s \n","\u001b[?25hRequirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.2.0)\n","Collecting protobuf~=3.13.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/79/510974552cebff2ba04038544799450defe75e96ea5f1675dbf72cc8744f/protobuf-3.13.0-cp36-cp36m-manylinux1_x86_64.whl (1.3MB)\n","\u001b[K     |████████████████████████████████| 1.3MB 37.0MB/s \n","\u001b[?25hRequirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.0)\n","Collecting tf-estimator-nightly~=2.4.0.dev\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/d2/2131f5a0f0d14bae7f4d332724748b9ca6746b0d32f5c76145f0707f47d8/tf_estimator_nightly-2.4.0.dev2020102301-py2.py3-none-any.whl (461kB)\n","\u001b[K     |████████████████████████████████| 471kB 52.6MB/s \n","\u001b[?25hCollecting numpy~=1.19.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/86/753182c9085ba4936c0076269a571613387cdb77ae2bf537448bfd63472c/numpy-1.19.4-cp36-cp36m-manylinux2010_x86_64.whl (14.5MB)\n","\u001b[K     |████████████████████████████████| 14.5MB 255kB/s \n","\u001b[?25hRequirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.15.0)\n","Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.10.0)\n","Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.1)\n","Collecting grpcio~=1.32.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/00/b393f5d0e92b37592a41357ea3077010c95400c907f6b9af01f4f6abe140/grpcio-1.32.0-cp36-cp36m-manylinux2014_x86_64.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 41.0MB/s \n","\u001b[?25hRequirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12)\n","Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.10.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (3.3.3)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (1.7.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (0.4.2)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (50.3.2)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (2.23.0)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (1.17.2)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (1.0.1)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tb-nightly~=2.5.0.a->tf-nightly) (2.0.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.5.0.a->tf-nightly) (1.3.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.5.0.a->tf-nightly) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.5.0.a->tf-nightly) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.5.0.a->tf-nightly) (2020.11.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.5.0.a->tf-nightly) (1.24.3)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.5.0.a->tf-nightly) (4.1.1)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.5.0.a->tf-nightly) (4.6)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.5.0.a->tf-nightly) (0.2.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly~=2.5.0.a->tf-nightly) (3.4.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.5.0.a->tf-nightly) (3.1.0)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tb-nightly~=2.5.0.a->tf-nightly) (0.4.8)\n","\u001b[31mERROR: tensorflow 2.3.0 has requirement numpy<1.19.0,>=1.16.0, but you'll have numpy 1.19.4 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: protobuf, numpy, grpcio, tb-nightly, tf-estimator-nightly, tf-nightly\n","  Found existing installation: protobuf 3.12.4\n","    Uninstalling protobuf-3.12.4:\n","      Successfully uninstalled protobuf-3.12.4\n","  Found existing installation: numpy 1.18.5\n","    Uninstalling numpy-1.18.5:\n","      Successfully uninstalled numpy-1.18.5\n","  Found existing installation: grpcio 1.33.2\n","    Uninstalling grpcio-1.33.2:\n","      Successfully uninstalled grpcio-1.33.2\n","Successfully installed grpcio-1.32.0 numpy-1.19.4 protobuf-3.13.0 tb-nightly-2.5.0a20201125 tf-estimator-nightly-2.4.0.dev2020102301 tf-nightly-2.5.0.dev20201125\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["google","numpy"]}}},"metadata":{"tags":[]}},{"output_type":"stream","text":["2020-11-25 13:56:28.218727: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2020-11-25 13:56:28.218770: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n","INFO line 93:22: tf.random_uniform_initializer requires manual check. Initializers no longer have the dtype argument in the constructor or partition_info argument in the __call__ method.\n","The calls have been converted to compat.v1 for safety (even though they may already have been correct).\n","INFO line 93:22: Renamed 'tf.random_uniform_initializer' to 'tf.compat.v1.random_uniform_initializer'\n","WARNING line 111:16: *.save requires manual check. (This warning is only applicable if the code saves a tf.Keras model) Keras model.save now saves to the Tensorflow SavedModel format by default, instead of HDF5. To continue saving to HDF5, add the argument save_format='h5' to the save() function.\n","TensorFlow 2.0 Upgrade Script\n","-----------------------------\n","Converted 1 files\n","Detected 1 issues that require attention\n","--------------------------------------------------------------------------------\n","--------------------------------------------------------------------------------\n","File: ./train.py\n","--------------------------------------------------------------------------------\n","./train.py:111:16: WARNING: *.save requires manual check. (This warning is only applicable if the code saves a tf.Keras model) Keras model.save now saves to the Tensorflow SavedModel format by default, instead of HDF5. To continue saving to HDF5, add the argument save_format='h5' to the save() function.\n","\n","\n","Make sure to read the detailed log 'report.txt'\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RYmMgArNlja9"},"source":["cp -r Model.py Model_test.py"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"tpO1LzyElmb9","executionInfo":{"status":"ok","timestamp":1606210435405,"user_tz":-420,"elapsed":72168,"user":{"displayName":"Tuấn Tô Huỳnh","photoUrl":"","userId":"00317117433846939709"}},"outputId":"045e8517-81b4-49e1-d12b-e7443389daa7"},"source":["!pip install tf-nightly\n","!tf_upgrade_v2 --infile ./Model_test.py --outfile ./Model_test_converted.py"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting tf-nightly\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/af/5c2da78d8035ff44e7d13102ddb01098574e138c0ed01fda3badf8b0338f/tf_nightly-2.5.0.dev20201124-cp36-cp36m-manylinux2010_x86_64.whl (397.6MB)\n","\u001b[K     |████████████████████████████████| 397.6MB 39kB/s \n","\u001b[?25hRequirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.2.0)\n","Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.3.0)\n","Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.0)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.35.1)\n","Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12)\n","Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.1)\n","Collecting tb-nightly~=2.5.0.a\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/09/f85156192f032722a6ad1a5abd33fe1b71dff1bf0ce3541fa08a8ae0f1a3/tb_nightly-2.5.0a20201124-py3-none-any.whl (11.9MB)\n","\u001b[K     |████████████████████████████████| 11.9MB 55.6MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.7.4.3)\n","Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.3.3)\n","Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.6.3)\n","Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.15.0)\n","Collecting grpcio~=1.32.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/00/b393f5d0e92b37592a41357ea3077010c95400c907f6b9af01f4f6abe140/grpcio-1.32.0-cp36-cp36m-manylinux2014_x86_64.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 53.0MB/s \n","\u001b[?25hRequirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.10.0)\n","Collecting protobuf~=3.13.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/79/510974552cebff2ba04038544799450defe75e96ea5f1675dbf72cc8744f/protobuf-3.13.0-cp36-cp36m-manylinux1_x86_64.whl (1.3MB)\n","\u001b[K     |████████████████████████████████| 1.3MB 43.6MB/s \n","\u001b[?25hCollecting numpy~=1.19.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/86/753182c9085ba4936c0076269a571613387cdb77ae2bf537448bfd63472c/numpy-1.19.4-cp36-cp36m-manylinux2010_x86_64.whl (14.5MB)\n","\u001b[K     |████████████████████████████████| 14.5MB 211kB/s \n","\u001b[?25hCollecting tf-estimator-nightly~=2.4.0.dev\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/d2/2131f5a0f0d14bae7f4d332724748b9ca6746b0d32f5c76145f0707f47d8/tf_estimator_nightly-2.4.0.dev2020102301-py2.py3-none-any.whl (461kB)\n","\u001b[K     |████████████████████████████████| 471kB 53.7MB/s \n","\u001b[?25hRequirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.10.0)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.2)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (1.7.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (50.3.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (3.3.3)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (0.4.2)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (1.0.1)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (1.17.2)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (2.23.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tb-nightly~=2.5.0.a->tf-nightly) (2.0.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.5.0.a->tf-nightly) (1.3.0)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.5.0.a->tf-nightly) (4.1.1)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.5.0.a->tf-nightly) (4.6)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.5.0.a->tf-nightly) (0.2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.5.0.a->tf-nightly) (2020.11.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.5.0.a->tf-nightly) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.5.0.a->tf-nightly) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.5.0.a->tf-nightly) (3.0.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly~=2.5.0.a->tf-nightly) (3.4.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.5.0.a->tf-nightly) (3.1.0)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tb-nightly~=2.5.0.a->tf-nightly) (0.4.8)\n","\u001b[31mERROR: tensorflow 2.3.0 has requirement numpy<1.19.0,>=1.16.0, but you'll have numpy 1.19.4 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: numpy, grpcio, protobuf, tb-nightly, tf-estimator-nightly, tf-nightly\n","  Found existing installation: numpy 1.18.5\n","    Uninstalling numpy-1.18.5:\n","      Successfully uninstalled numpy-1.18.5\n","  Found existing installation: grpcio 1.33.2\n","    Uninstalling grpcio-1.33.2:\n","      Successfully uninstalled grpcio-1.33.2\n","  Found existing installation: protobuf 3.12.4\n","    Uninstalling protobuf-3.12.4:\n","      Successfully uninstalled protobuf-3.12.4\n","Successfully installed grpcio-1.32.0 numpy-1.19.4 protobuf-3.13.0 tb-nightly-2.5.0a20201124 tf-estimator-nightly-2.4.0.dev2020102301 tf-nightly-2.5.0.dev20201124\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["google","numpy"]}}},"metadata":{"tags":[]}},{"output_type":"stream","text":["2020-11-24 16:33:48.275721: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2020-11-24 16:33:48.275758: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n","INFO line 17:20: Renamed 'tf.contrib.rnn.BasicLSTMCell' to 'tf.compat.v1.nn.rnn_cell.BasicLSTMCell'\n","WARNING line 19:24: Using member tf.contrib.rnn.DropoutWrapper in deprecated module tf.contrib.rnn. (Manual edit required) tf.contrib.rnn.* has been deprecated, and widely used cells/functions will be moved to tensorflow/addons repository. Please check it there and file Github issues if necessary.\n","ERROR line 19:24: Using member tf.contrib.rnn.DropoutWrapper in deprecated module tf.contrib. tf.contrib.rnn.DropoutWrapper cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\n","INFO line 21:15: Renamed 'tf.contrib.rnn.MultiRNNCell' to 'tf.compat.v1.nn.rnn_cell.MultiRNNCell'\n","INFO line 28:21: Added keywords to args of function 'tf.nn.embedding_lookup'\n","WARNING line 28:21: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n","INFO line 31:21: Changing keep_prob arg of tf.nn.dropout to rate, and recomputing value.\n","\n","ERROR line 54:15: Using member tf.contrib.legacy_seq2seq.sequence_loss_by_example in deprecated module tf.contrib. tf.contrib.legacy_seq2seq.sequence_loss_by_example cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\n","INFO line 58:28: Added keywords to args of function 'tf.reduce_sum'\n","INFO line 61:42: Added keywords to args of function 'tf.gradients'\n","TensorFlow 2.0 Upgrade Script\n","-----------------------------\n","Converted 1 files\n","Detected 4 issues that require attention\n","--------------------------------------------------------------------------------\n","--------------------------------------------------------------------------------\n","File: ./Model_test.py\n","--------------------------------------------------------------------------------\n","./Model_test.py:19:24: WARNING: Using member tf.contrib.rnn.DropoutWrapper in deprecated module tf.contrib.rnn. (Manual edit required) tf.contrib.rnn.* has been deprecated, and widely used cells/functions will be moved to tensorflow/addons repository. Please check it there and file Github issues if necessary.\n","./Model_test.py:19:24: ERROR: Using member tf.contrib.rnn.DropoutWrapper in deprecated module tf.contrib. tf.contrib.rnn.DropoutWrapper cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\n","./Model_test.py:28:21: WARNING: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n","./Model_test.py:54:15: ERROR: Using member tf.contrib.legacy_seq2seq.sequence_loss_by_example in deprecated module tf.contrib. tf.contrib.legacy_seq2seq.sequence_loss_by_example cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\n","\n","\n","Make sure to read the detailed log 'report.txt'\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eVKntsCYmnh8"},"source":["!mv Model.py Model_bk.py\n","!mv Model_test_converted.py Model.py"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pOydAqP8loPQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606210590157,"user_tz":-420,"elapsed":9894,"user":{"displayName":"Tuấn Tô Huỳnh","photoUrl":"","userId":"00317117433846939709"}},"outputId":"455d76f0-ca02-4f5c-8b26-be7b555d0d89"},"source":["#Giai_doan1:\n","!python3 lyric_generator_part1.py \"anh 50 False\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["2020-11-24 16:36:20.692955: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2020-11-24 16:36:20.692998: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n","Load_model: \n","2020-11-24 16:36:22.640294: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2020-11-24 16:36:22.640444: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n","2020-11-24 16:36:22.641436: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n","2020-11-24 16:36:22.674685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-11-24 16:36:22.675250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n","coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\n","2020-11-24 16:36:22.675372: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2020-11-24 16:36:22.675530: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2020-11-24 16:36:22.676757: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2020-11-24 16:36:22.678357: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n","2020-11-24 16:36:22.678723: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n","2020-11-24 16:36:22.689335: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n","2020-11-24 16:36:22.690065: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2020-11-24 16:36:22.690203: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2020-11-24 16:36:22.690227: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1764] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n","Skipping registering GPU devices...\n","2020-11-24 16:36:22.795783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-11-24 16:36:22.795825: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1273]      0 \n","2020-11-24 16:36:22.795840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1286] 0:   N \n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:702: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n","  warnings.warn(\"`tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be \"\n","WARNING:tensorflow:<tensorflow.python.keras.layers.legacy_rnn.rnn_cell_impl.BasicLSTMCell object at 0x7f4f42427f98>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n","W1124 16:36:22.803332 139979745699712 rnn_cell_impl.py:712] <tensorflow.python.keras.layers.legacy_rnn.rnn_cell_impl.BasicLSTMCell object at 0x7f4f42427f98>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n","WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n","W1124 16:36:22.806829 139979745699712 rnn_cell_impl.py:1242] `tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:At least two cells provided to MultiRNNCell are the same object and will share weights.\n","W1124 16:36:22.807848 139979745699712 rnn_cell_impl.py:1254] At least two cells provided to MultiRNNCell are the same object and will share weights.\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1702: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n","  warnings.warn('`layer.add_variable` is deprecated and '\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:753: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","W1124 16:36:22.832208 139979745699712 deprecation.py:534] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:753: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","model loading ...\n","INFO:tensorflow:Restoring parameters from ./model/Model-40\n","I1124 16:36:22.886055 139979745699712 saver.py:1297] Restoring parameters from ./model/Model-40\n","2020-11-24 16:36:22.887239: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:252] None of the MLIR optimization passes are enabled (registered 0 passes)\n","2020-11-24 16:36:22.888847: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2200000000 Hz\n","Done!\n","char:  anh\n","Generated Result here2:\n","\n"," anh đã từng 2128 yêu \n"," 2128 xin em hãy quên đi tình 2157 yêu em \n"," và hãy quên 1845 em đi \n"," hãy quên đi tình 2128 yêu này \n"," 2157 đừng quên anh nhé 2128 em , đừng đi nữa 1845 em ơi \n"," xin đừng để 2128\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mRVC0YS3gOjt","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1575959377758,"user_tz":-420,"elapsed":8038,"user":{"displayName":"Tuấn Tô Huỳnh","photoUrl":"","userId":"00317117433846939709"}},"outputId":"db509d31-f473-4889-9930-ec08ba242e39"},"source":["# Giai_đoạn 2:\n","#B1: Sinh bảng cảm âm\n","!python3 cam_am.py \"mi 50 True\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Load_model: \n","2019-12-10 06:29:30.135778: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n","2019-12-10 06:29:30.135969: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2352bc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2019-12-10 06:29:30.136002: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2019-12-10 06:29:30.137964: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2019-12-10 06:29:30.243346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-12-10 06:29:30.244075: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2352d80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2019-12-10 06:29:30.244107: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2019-12-10 06:29:30.244300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-12-10 06:29:30.244835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2019-12-10 06:29:30.245144: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2019-12-10 06:29:30.246456: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2019-12-10 06:29:30.247930: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2019-12-10 06:29:30.248233: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2019-12-10 06:29:30.249562: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2019-12-10 06:29:30.250306: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2019-12-10 06:29:30.253209: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2019-12-10 06:29:30.253316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-12-10 06:29:30.253854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-12-10 06:29:30.254338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n","2019-12-10 06:29:30.254389: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2019-12-10 06:29:30.255358: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2019-12-10 06:29:30.255386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n","2019-12-10 06:29:30.255397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n","2019-12-10 06:29:30.255489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-12-10 06:29:30.256058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-12-10 06:29:30.256579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","W1210 06:29:30.259150 140158150309760 lazy_loader.py:50] \n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","I1210 06:29:30.702388 140158150309760 utils.py:141] NumExpr defaulting to 2 threads.\n","WARNING:tensorflow:From /content/drive/My Drive/LuanVan/tom_chang_new/Model.py:17: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n","W1210 06:29:31.030277 140158150309760 deprecation.py:323] From /content/drive/My Drive/LuanVan/tom_chang_new/Model.py:17: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f78c8c33668>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n","W1210 06:29:31.030838 140158150309760 rnn_cell_impl.py:698] <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f78c8c33668>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n","WARNING:tensorflow:From /content/drive/My Drive/LuanVan/tom_chang_new/Model.py:21: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n","W1210 06:29:31.032125 140158150309760 deprecation.py:323] From /content/drive/My Drive/LuanVan/tom_chang_new/Model.py:21: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:At least two cells provided to MultiRNNCell are the same object and will share weights.\n","W1210 06:29:31.032643 140158150309760 rnn_cell_impl.py:1235] At least two cells provided to MultiRNNCell are the same object and will share weights.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.add_weight` method instead.\n","W1210 06:29:31.051788 140158150309760 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.add_weight` method instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","W1210 06:29:31.058353 140158150309760 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","model cam_am loading ...\n","INFO:tensorflow:Restoring parameters from ./cam_am/model_cam_am/Model-40\n","I1210 06:29:31.124585 140158150309760 saver.py:1284] Restoring parameters from ./cam_am/model_cam_am/Model-40\n","Model_cam_cam: Done!\n","2019-12-10 06:29:31.234311: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","char:  mi\n","[219  88]\n","[107 219]\n","['la', 'đô']\n","[(-0.3393755257129669, ['mi', 'đô'], 219, array([[ 4.5762295e-03, -1.6888034e-02,  2.1980430e-01, ...,\n","         6.4101785e-01, -2.3337464e-01, -6.2028226e-04]], dtype=float32)), (-2.356712818145752, ['mi', 'la'], 107, array([[ 4.5762295e-03, -1.6888034e-02,  2.1980430e-01, ...,\n","         6.4101785e-01, -2.3337464e-01, -6.2028226e-04]], dtype=float32))]\n","Input: \n"," +result_begin_with:  mi\n"," +with_len:  50\n"," +using Beams search:  True\n","Generated Result here:\n","51\n","path_to_file_cam_am:  cam_am/Result_cam_am/mi_50_True.txt\n","mi đô son mison \n"," la đô rê mi rê mi \n"," son la đô mi mi rê đô rê \n"," rê pha son la , son pha son pha mi rê \n"," rê pha son la , son la son pha rê \n"," rê rê pha son , son la son pha rê\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GBkggOOD_546","colab":{"base_uri":"https://localhost:8080/","height":224},"executionInfo":{"status":"ok","timestamp":1575959399495,"user_tz":-420,"elapsed":1376,"user":{"displayName":"Tuấn Tô Huỳnh","photoUrl":"","userId":"00317117433846939709"}},"outputId":"da86f902-5041-48c3-dfb7-3845a8da2c92"},"source":["#B2: Sinh nhịp 2/4 cho bảng cảm âm:\n","import module\n","name_file=\"mi_50_True.txt\"\n","path='cam_am/Result_cam_am/'+name_file\n","path_nhip='cam_am/Result_cam_am_nhip/'+name_file\n","name=path.split('/')[2]\n","with open (path,'r',encoding='utf-8') as f:\n","  data=f.read()\n","print(data)\n","add_nhip=module.add_nhip(path)\n","# with open ('cam_am/Result_cam_am_nhip/mi_100_True.txt','r',encoding='utf-8') as f:\n","#   data=f.read()\n","# print(data)\n","with open ('cam_am/Result_cam_am_nhip/'+name,'w',encoding='utf-8') as f:\n","  f.write(add_nhip)\n","print(add_nhip)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["mi đô son mison \n"," la đô rê mi rê mi \n"," son la đô mi mi rê đô rê \n"," rê pha son la , son pha son pha mi rê \n"," rê pha son la , son la son pha rê \n"," rê rê pha son , son la son pha rê\n","16mi 16đô 8son 4mison || \n","4la 4đô ||  4rê 8mi 8rê ||  2mi || \n","4son 4la ||  4đô 4mi ||  8mi 8rê 4đô ||  2rê || \n","8rê 8pha 4son ||  2la ||  , 4son 4pha ||  4son 4pha ||  4mi 4rê || \n","16rê 16pha 8son 4la ||  , 4son 8la 8son ||  4pha 4rê || \n","8rê 8rê 4pha ||  2son ||  , 4son 8la 8son ||  4pha 4rê || \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Y265T-duro5z","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1575959436021,"user_tz":-420,"elapsed":8389,"user":{"displayName":"Tuấn Tô Huỳnh","photoUrl":"","userId":"00317117433846939709"}},"outputId":"aeba5d60-b84d-422f-dfb8-9b386a61ef06"},"source":["#B3 tạo lời bài hát và hợp âm dựa trên bảng cảm âm:\n","!python3 lyric_generator_part2.py \"anh mi_50_True.txt True\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Load_model: \n","mi đô son mison \n"," la đô rê mi rê mi \n"," son la đô mi mi rê đô rê \n"," rê pha son la , son pha son pha mi rê \n"," rê pha son la , son la son pha rê \n"," rê rê pha son , son la son pha rê\n","2019-12-10 06:30:28.287406: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n","2019-12-10 06:30:28.287566: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3132bc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2019-12-10 06:30:28.287594: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2019-12-10 06:30:28.289481: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2019-12-10 06:30:28.391534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-12-10 06:30:28.392265: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3132d80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2019-12-10 06:30:28.392295: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2019-12-10 06:30:28.392436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-12-10 06:30:28.392963: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2019-12-10 06:30:28.393220: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2019-12-10 06:30:28.394639: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2019-12-10 06:30:28.396226: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2019-12-10 06:30:28.396522: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2019-12-10 06:30:28.398008: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2019-12-10 06:30:28.398707: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2019-12-10 06:30:28.401617: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2019-12-10 06:30:28.401707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-12-10 06:30:28.402314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-12-10 06:30:28.402812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n","2019-12-10 06:30:28.402891: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2019-12-10 06:30:28.403961: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2019-12-10 06:30:28.403992: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n","2019-12-10 06:30:28.404001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n","2019-12-10 06:30:28.404104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-12-10 06:30:28.404658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-12-10 06:30:28.405188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","W1210 06:30:28.408930 139677830739840 lazy_loader.py:50] \n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","I1210 06:30:28.843332 139677830739840 utils.py:141] NumExpr defaulting to 2 threads.\n","WARNING:tensorflow:From /content/drive/My Drive/LuanVan/tom_chang_new/Model.py:17: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n","W1210 06:30:29.176182 139677830739840 deprecation.py:323] From /content/drive/My Drive/LuanVan/tom_chang_new/Model.py:17: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f08f30fff98>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n","W1210 06:30:29.176700 139677830739840 rnn_cell_impl.py:698] <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f08f30fff98>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n","WARNING:tensorflow:From /content/drive/My Drive/LuanVan/tom_chang_new/Model.py:21: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n","W1210 06:30:29.177906 139677830739840 deprecation.py:323] From /content/drive/My Drive/LuanVan/tom_chang_new/Model.py:21: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:At least two cells provided to MultiRNNCell are the same object and will share weights.\n","W1210 06:30:29.178365 139677830739840 rnn_cell_impl.py:1235] At least two cells provided to MultiRNNCell are the same object and will share weights.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.add_weight` method instead.\n","W1210 06:30:29.199486 139677830739840 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.add_weight` method instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","W1210 06:30:29.205989 139677830739840 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","model loading ...\n","INFO:tensorflow:Restoring parameters from ./model/Model-40\n","I1210 06:30:29.270111 139677830739840 saver.py:1284] Restoring parameters from ./model/Model-40\n","Done!\n","2019-12-10 06:30:29.614782: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","created beams with char:  anh\n","input: \n","start_sentence:  anh\n","path_to_ascoutics_panel:  mi_50_True.txt\n","is beams:  True\n","*****************************************************\n","Lyrics generated: \n"," anh trong một 2128 đôi \n"," 2128 đi về bên 2199 anh 2128 có 2128 ai \n"," có 2128 lúc ta đi xa nhau thì em \n"," mình vẫn còn mãi , dù cho ai có biết gì \n"," như mưa từng đêm , em chờ anh từng đêm \n"," anh vẫn chờ em , chờ em đợi anh trong\n","*****************************************************\n","Lyrics and chord generated base on ascoutics panel: \n"," anh trong một 2128 đôi \n","16mi 16đô 8son 4mison || \n"," 2128 đi về bên 2199 anh 2128 có 2128 ai \n","4la 4đô ||  4rê 8mi 8rê ||  2mi || \n"," có 2128 lúc ta đi xa nhau thì em \n","4son 4la ||  4đô 4mi ||  8mi 8rê 4đô ||  2rê || \n"," mình vẫn còn mãi , dù cho ai có biết gì \n","8rê 8pha 4son ||  2la ||  , 4son 4pha ||  4son 4pha ||  4mi 4rê || \n"," như mưa từng đêm , em chờ anh từng đêm \n","16rê 16pha 8son 4la ||  , 4son 8la 8son ||  4pha 4rê || \n"," anh vẫn chờ em , chờ em đợi anh trong\n","8rê 8rê 4pha ||  2son ||  , 4son 8la 8son ||  4pha 4rê || \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"22-FM_rS_-yU","colab":{"base_uri":"https://localhost:8080/","height":315},"executionInfo":{"status":"error","timestamp":1575913060633,"user_tz":-420,"elapsed":9299,"user":{"displayName":"Tuấn Tô Huỳnh","photoUrl":"","userId":"00317117433846939709"}},"outputId":"d21a5d3e-7619-44b5-c65d-1665a7442f1c"},"source":["#test_new_idea\n","#coding:utf-8\n","import tensorflow as tf\n","import sys,time\n","import numpy as np\n","import pickle as cPickle\n","import os\n","import random\n","import Config\n","import Model\n","import codecs\n","import module\n","from random import choice\n","config_tf = tf.compat.v1.ConfigProto()\n","config_tf.gpu_options.allow_growth = True\n","config_tf.inter_op_parallelism_threads = 1\n","config_tf.intra_op_parallelism_threads = 1\n","#set_up_cac_gia_tri:\n","list_word_before_dot=module.get_list_before_dot_or_phay('before_dot.csv')\n","list_word_before_phay=module.get_list_before_dot_or_phay('before_phay.csv')\n","list_2word_before_dot=module.get_list_before_dot_or_phay('file_config/before_dot2.csv')\n","list_2word_before_phay=module.get_list_before_dot_or_phay('file_config/before_phay2.csv')\n","list_dong_am=['mãi mãi','luôn luôn']\n","list_ba_tu_lap=['em yêu em','anh yêu anh','yêu em yêu','yêu anh yêu','em ơi em','anh ơi anh']\n","list_bon_tu_lap=[\"anh vẫn yêu anh\",'em vẫn yêu em','anh vẫn mong anh','em vẫn mong em','em đã yêu em',\n","                  \"anh rất yêu anh\",\"em rất yêu em\",'anh đã yêu anh','em thật nhiều em ','em sẽ yêu em',\n","                  'anh sẽ yêu anh','em chỉ cần em','anh chỉ cần anh']\n","config = Config.Config()\n","print(\"Load_model: \")\n","char_to_idx, idx_to_char = cPickle.load(open(config.model_path+'.voc', 'rb'))\n","\n","config.vocab_size = len(char_to_idx)\n","# len_of_generation = config.len_of_generation\n","#khong trừ 1 vì cần insert 1 từ để tạo beam\n","if (len(sys.argv) == 2):\n","    if (sys.version_info > (3, 0)):\n","        input_ = sys.argv[1]\n","    else:\n","        input_ = sys.argv[1].decode(\"utf-8\")\n","# input_=\"mùa đông mi_50_True.txt True\"\n","list_input=input_.split()\n","len_=len(list_input)\n","#khoi tạo input\n","start_sentence=' '.join(list_input[:len_-2])+' 100'\n","is_beams = list_input[len_-1]\n","len_start_idx=len(start_sentence.split())\n","#doc bang cam am\n","filename='cam_am/Result_cam_am/'+list_input[len_-2]\n","filename_nhip='cam_am/Result_cam_am_nhip/'+list_input[len_-2]\n","is_sample = config.is_sample\n","beam_size = config.beam_size\n","def read_file(path):\n","  with open(path, encoding='utf-8') as f:\n","      data = f.read()\n","  return data\n","def check_is_chord(word):\n","  check=False\n","  if(word[0] in ['0','1','2','3','4','5','6','7']):\n","    check=True\n","  return check\n","data=read_file(filename)\n","data_nhip=read_file(filename_nhip)\n","# data=data.split('\\n')[0]+'\\n'+data.split('\\n')[1]+'\\n'+data.split('\\n')[2]+'\\n'+data.split('\\n')[3]\n","print(data)\n","list_sentence_cam_am=data.split('\\n')\n","list_sentence_cam_am_nhip=data_nhip.split('\\n')\n","def get_len(sentence):\n","  return sentence.split()\n","def dem_word(sentence):\n","  list_word=sentence.split()\n","  list_word_not_chord=[]\n","  for word in list_word:\n","    if(word[0] not in ['0','1','2','3','4','5','6','7']):\n","      list_word_not_chord=list_word_not_chord+[word]\n","  # print(\"dem word trong cau: \",sentence,' ketqua là: ',dem)\n","  return list_word_not_chord\n","list_cam_am=(data.replace('\\n','.')+' .').split()\n","len_list_cam_am=len(list_cam_am)\n","# print(list_cam_am)\n","# print(len_list_cam_am)\n","\n","def run_epoch(session, m, data, eval_op, state=None):\n","    \"\"\"Runs the model on the given data.\"\"\"\n","    x = data.reshape((1,1))\n","    prob, _state, _ = session.run([m._prob, m.final_state, eval_op],\n","                         {m.input_data: x,\n","                          m.initial_state: state})\n","    return prob, _state\n","\n","def main(_):\n","    with tf.Graph().as_default(), tf.compat.v1.Session(config=config_tf) as session:\n","        config.batch_size = 1\n","        config.num_steps = 1\n","\n","        initializer = tf.random_uniform_initializer(-config.init_scale,\n","                                                config.init_scale)\n","        with tf.compat.v1.variable_scope(\"model\", reuse=None, initializer=initializer):\n","            mtest = Model.Model(is_training=False, config=config)\n","\n","        #tf.global_variables_initializer().run()\n","\n","        model_saver = tf.compat.v1.train.Saver()\n","        print(\"model loading ...\")\n","        model_saver.restore(session, config.model_path+'-%d'%config.save_time)\n","        # model_saver.restore(session, config.model_path+str(-5))\n","        print (\"Done!\")\n","        char_list = (start_sentence.split())\n","        word_before_word_begin=''\n","        if(len(char_list)>2):\n","            word_before_word_begin=' '.join(char_list[0:len(char_list)-2])\n","        # word=' '.join(char_list[0:len(char_list)-1])\n","        word=char_list[len(char_list)-2]\n","        len_of_generation=len_list_cam_am\n","        if not is_beams:\n","            # sentence state\n","            # char_list = (start_sentence.split());\n","            # start_idx = char_to_idx[char_list[0]]\n","            start_idx = char_to_idx[word]\n","            _state = mtest.initial_state.eval()\n","            test_data = np.int32([start_idx])\n","            prob, _state = run_epoch(session, mtest, test_data, tf.no_op(), _state)\n","            # gen_res = [char_list[0]]\n","            gen_res = []\n","            # for i in range(1, len(char_list)):\n","            # for i in range(1, len(word)):\n","            char = word\n","            try:\n","                char_index = char_to_idx[char]\n","            except KeyError:\n","                char_index = np.argmax(prob.reshape(-1))\n","            prob, _state = run_epoch(session, mtest, np.int32([char_index]), tf.no_op(), _state)\n","            gen_res.append(char)\n","            # gen text\n","            if is_sample:\n","                gen = np.random.choice(config.vocab_size, 1, p=prob.reshape(-1))\n","                gen = gen[0]\n","            else:\n","                gen = np.argmax(prob.reshape(-1))\n","            test_data = np.int32(gen)\n","            gen_res.append(idx_to_char[gen])\n","            for i in range(len_of_generation-1):\n","                prob, _state = run_epoch(session, mtest, test_data, tf.no_op(), _state)\n","                if is_sample:\n","                    gen = np.random.choice(config.vocab_size, 1, p=prob.reshape(-1))\n","                    gen = gen[0]\n","                else:\n","                    gen = np.argmax(prob.reshape(-1))\n","                test_data = np.int32(gen)\n","                gen_res.append(idx_to_char[gen])\n","            # print(gen_res)\n","            print(\"Generated Result here:\")\n","            # print('ketqua: ')\n","            print((word_before_word_begin+(' ').join(gen_res)).replace('.','\\n'))\n","            with open('Result/'+str(get_number_file_result+1)+'.txt','a',encoding='utf-8') as file1:\n","                file1.write((word_before_word_begin+(' ').join(gen_res)).replace('.','\\n'))\n","        else:\n","            # sentence state\n","            # char_list = (start_sentence).split();\n","            # start_idx = char_to_idx[char_list[0]]\n","            start_idx = char_to_idx[word]\n","            _state = mtest.initial_state.eval()\n","            beams = [(0.0, [idx_to_char[start_idx]], idx_to_char[start_idx])]\n","            test_data = np.int32([start_idx])\n","            prob, _state = run_epoch(session, mtest, test_data, tf.no_op(), _state)\n","            y1 = np.log(1e-20 + prob.reshape(-1))\n","            beams = [(beams[0][0], beams[0][1], beams[0][2], _state)]\n","            # for i in range(1, len(char_list)):\n","            # for i in range(1, len(word)):\n","            char = word\n","            print(\"created beams with char: \",char)\n","            try:\n","                char_index = char_to_idx[char]\n","            except KeyError:\n","                top_indices = np.argsort(-y1)\n","                char_index = top_indices[0]\n","            prob, _state = run_epoch(session, mtest, np.int32([char_index]), tf.no_op(), beams[0][3])\n","            y1 = np.log(1e-20 + prob.reshape(-1))\n","            beams = [(beams[0][0], beams[0][1], char_index, _state)]\n","            if(list_cam_am[len(char_list)-1]!='.' or list_cam_am[len(char_list)-1]!=','):\n","              status=True\n","              while(status):\n","                if is_sample:\n","                    top_indices = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","                else:\n","                    top_indices = np.argsort(-y1)\n","                if(idx_to_char[top_indices[0]]=='.' or idx_to_char[top_indices[1]]=='.'):\n","                  not_process=1\n","                elif(idx_to_char[top_indices[0]]==',' or idx_to_char[top_indices[1]]==','):\n","                  not_process=1\n","                else:\n","                  status=False\n","            else:\n","              if is_sample:\n","                    top_indices = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","              else:\n","                    top_indices = np.argsort(-y1)\n","            b = beams[0]\n","            beam_candidates = []\n","            for i in range(beam_size):\n","                wordix = top_indices[i]\n","                beam_candidates.append((b[0] + y1[wordix], b[1] + [idx_to_char[wordix]], wordix, _state))\n","            beam_candidates.sort(key = lambda x:x[0], reverse = True) # decreasing order\n","            beam_new=[]\n","            for nhien in beam_candidates:\n","              # print(nhien)\n","              word=idx_to_char[nhien[2]]\n","              if(check_is_chord(word)==True):\n","                # print(nhien[1][len(nhien[1])-2])\n","                idx=char_to_idx[nhien[1][len(nhien[1])-2]]\n","                test_data = np.int32(idx)\n","                prob, _state = run_epoch(session, mtest, test_data, tf.no_op(), b[3])\n","                y1 = np.log(1e-20 + prob.reshape(-1))\n","                check_accept=True\n","                while(check_accept):\n","                  if is_sample:\n","                      top_indices = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","                  else:\n","                      top_indices = np.argsort(-y1)\n","                  word1=idx_to_char[top_indices[0]]\n","                  word2=idx_to_char[top_indices[1]]\n","                  # print(nhien[1])\n","                  list_word=[word1,word2]\n","                  for x in list_word:\n","                    if(check_is_chord(x)==False and x!=',' and x!='.'):\n","                      check_accept=False\n","                      word_ok=x\n","                      break\n","                # beam_new.append((nhien[0] + y1[char_to_idx[word_ok]], nhien[1] + [word_ok], char_to_idx[word_ok], _state))\n","                beam_new.append((nhien[0], nhien[1] + [word_ok], char_to_idx[word_ok], _state))\n","              else:\n","                beam_new.append(nhien)\n","            # print(beam_new)\n","            beam_candidates=beam_new\n","            beams = beam_candidates[:beam_size] # truncate to get new beams\n","            # print(beams)\n","            # print(list_cam_am[1])\n","            #chạy từ 2 vì word_begin+first_word_using_for_create_beam\n","            for xy in range(len_start_idx,len_of_generation-1,1):\n","                beam_candidates = []\n","                # print(list_cam_am[xy])\n","                if(list_cam_am[xy]=='.'):\n","                  # print('in here')\n","                  for b in beams:\n","                      test_data = np.int32(b[2])\n","                      prob, _state = run_epoch(session, mtest, test_data, tf.no_op(), b[3])\n","                      y1 = np.log(1e-20 + prob.reshape(-1))\n","                      if is_sample:\n","                            top_indices = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","                      else:\n","                          top_indices = np.argsort(-y1)\n","                      if(idx_to_char[top_indices[0]]=='.' or idx_to_char[top_indices[1]]=='.'):\n","                        for i in range(beam_size):\n","                          wordix = top_indices[i]\n","                          beam_candidates.append((b[0] + y1[wordix], b[1] + [idx_to_char[wordix]], char_to_idx['.'], _state))\n","                      else:\n","                        for i in range(beam_size):\n","                          wordix = char_to_idx['.']\n","                          beam_candidates.append((b[0] + y1[wordix], b[1] + [idx_to_char[wordix]], wordix, _state))\n","                elif(list_cam_am[xy]==','):\n","                  for b in beams:\n","                      test_data = np.int32(b[2])\n","                      prob, _state = run_epoch(session, mtest, test_data, tf.no_op(), b[3])\n","                      y1 = np.log(1e-20 + prob.reshape(-1))\n","                      if is_sample:\n","                            top_indices = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","                      else:\n","                          top_indices = np.argsort(-y1)\n","                      if(idx_to_char[top_indices[0]]==',' or idx_to_char[top_indices[1]]==','):\n","                        for i in range(beam_size):\n","                          wordix = top_indices[i]\n","                          beam_candidates.append((b[0] + y1[wordix], b[1] + [idx_to_char[wordix]], wordix, _state))\n","                      else:\n","                        for i in range(beam_size):\n","                          wordix = char_to_idx[',']\n","                          beam_candidates.append((b[0] + y1[wordix], b[1] + [idx_to_char[wordix]], wordix, _state))\n","                else:\n","                  for b in beams:\n","                      test_data = np.int32(b[2])\n","                      prob, _state = run_epoch(session, mtest, test_data, tf.no_op(), b[3])\n","                      y1 = np.log(1e-20 + prob.reshape(-1))\n","                      status=True \n","                      while(status):\n","                        if is_sample:\n","                            top_indices = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","                        else:\n","                            top_indices = np.argsort(-y1)\n","                        if(idx_to_char[top_indices[0]]=='.' or idx_to_char[top_indices[1]]=='.'):\n","                          not_accept=1\n","                        elif(idx_to_char[top_indices[0]]==',' or idx_to_char[top_indices[1]]==','):\n","                          not_accept=1\n","                        else:\n","                          status=False\n","                      # print(idx_to_char[top_indices[0]],idx_to_char[top_indices[1]])\n","                      for i in range(beam_size):\n","                          wordix = top_indices[i]\n","                          beam_candidates.append((b[0] + y1[wordix], b[1] + [idx_to_char[wordix]], wordix, _state))\n","                beam_candidates.sort(key = lambda x:x[0], reverse = True) # decreasing order\n","                #deploy new idea:\n","                # print(\"beam original\")\n","                # print(beam_candidates)\n","                # print('==============================================')\n","                beam_new=[]\n","                for nhien in beam_candidates:\n","                  # print(nhien)\n","                  word=idx_to_char[nhien[2]]\n","                  if(check_is_chord(word)==True):\n","                    # print(nhien[1][len(nhien[1])-2])\n","                    idx=char_to_idx[nhien[1][len(nhien[1])-2]]\n","                    test_data = np.int32(idx)\n","                    prob, _state = run_epoch(session, mtest, test_data, tf.no_op(), b[3])\n","                    y1 = np.log(1e-20 + prob.reshape(-1))\n","                    Nhien_is_my_friendgirl=True\n","                    while(Nhien_is_my_friendgirl):\n","                      if is_sample:\n","                          top_indices = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","                      else:\n","                          top_indices = np.argsort(-y1)\n","                      word1=idx_to_char[top_indices[0]]\n","                      word2=idx_to_char[top_indices[1]]\n","                      # print(nhien[1])\n","                      list_word=[word1,word2]\n","                      for x in list_word:\n","                        if(check_is_chord(x)==False and x!=',' and x!='.'):\n","                          Nhien_is_my_friendgirl=False\n","                          word_ok=x\n","                          break\n","                    # beam_new.append((nhien[0] + y1[char_to_idx[word_ok]], nhien[1] + [word_ok], char_to_idx[word_ok], _state))\n","                    beam_new.append((nhien[0], nhien[1] + [word_ok], char_to_idx[word_ok], _state))\n","                  else:\n","                    beam_new.append(nhien)\n","                # print(beam_new)\n","                beam_candidates=beam_new\n","                # print(\"beam_candidates new: \\n\",beam_candidates)\n","                # beams=beam_candidates[:beam_size]\n","                # print(\"beam_new: \")\n","                # print(nhien)\n","                if(list_cam_am[xy]=='.'):\n","                  list_accept_beam=[]\n","                  for beam_check in beam_candidates:\n","                    list_not_chord=module.get_list_not_chord(beam_check[1])\n","                    # print(list_not_chord)\n","                    mot_tu_cuoi=module.get_last_string(list_not_chord,1)\n","                    hai_tu_cuoi=module.get_last_string(list_not_chord,2)\n","                    ba_tu_cuoi=module.get_last_string(list_not_chord,3)\n","                    mot_tu_before_dot=hai_tu_cuoi.split()[0]\n","                    hai_tu_before_dot=ba_tu_cuoi.split()[0]+' '+ba_tu_cuoi.split()[1] \n","                    if(mot_tu_cuoi=='.' and hai_tu_before_dot in list_2word_before_dot):\n","                      list_accept_beam.append(beam_check)\n","                    elif(mot_tu_cuoi=='.' and mot_tu_before_dot in list_word_before_dot and hai_tu_before_dot not in list_2word_before_dot):\n","                      list_accept_beam.append(beam_check)\n","                  # if(len(list_accept_beam)!=4):\n","                  #   print('list_accpet_beam',list_accept_beam)\n","                  list_accept_beam.sort(key = lambda x:x[0], reverse = True)\n","                  beams=list_accept_beam[:beam_size]\n","                elif(list_cam_am[xy]==','):\n","                  list_accept_beam=[]\n","                  for beam_check in beam_candidates:\n","                    list_not_chord=module.get_list_not_chord(beam_check[1])\n","                    mot_tu_cuoi=module.get_last_string(list_not_chord,1)\n","                    hai_tu_cuoi=module.get_last_string(list_not_chord,2)\n","                    ba_tu_cuoi=module.get_last_string(list_not_chord,3)\n","                    mot_tu_before_phay=hai_tu_cuoi.split()[0]\n","                    hai_tu_before_phay=ba_tu_cuoi.split()[0]+' '+ba_tu_cuoi.split()[1] \n","                    if(mot_tu_cuoi==',' and hai_tu_before_phay in list_word_before_phay):\n","                      list_accept_beam.append(beam_check)\n","                    elif(mot_tu_cuoi==',' and mot_tu_before_phay in list_word_before_phay and hai_tu_before_phay not in list_word_before_phay):\n","                      list_accept_beam.append(beam_check)\n","                  list_accept_beam.sort(key = lambda x:x[0], reverse = True)\n","                  beams=list_accept_beam[:beam_size]\n","                else:\n","                  # hay bo từ khi từ 4 == từ 1 nhỉ?\n","                  list_accept_beam=[]\n","                  for beam_check in beam_candidates:\n","                    list_not_chord=module.get_list_not_chord(beam_check[1])\n","                    mot_tu_cuoi=module.get_last_string(list_not_chord,1)\n","                    hai_tu_cuoi=module.get_last_string(list_not_chord,2)\n","                    # print(hai_tu_cuoi)\n","                    ba_tu_cuoi=module.get_last_string(list_not_chord,3)\n","                    bon_tu_cuoi=module.get_last_string(list_not_chord,4)\n","                    # print(ba_tu_cuoi)\n","                    if(mot_tu_cuoi=='.'):\n","                      kolamgi=1\n","                    elif(hai_tu_cuoi not in list_dong_am and hai_tu_cuoi.split()[0]==hai_tu_cuoi.split()[1] or hai_tu_cuoi=='anh em' or hai_tu_cuoi=='em anh'):\n","                      kolamgi=1\n","                    elif(ba_tu_cuoi in list_ba_tu_lap):\n","                      kolamgi=1\n","                    elif(bon_tu_cuoi in list_bon_tu_lap):\n","                      kolamgi=1\n","                    # test idea xet theo tu cuoi,tu cách cuoi 1 dv\n","                    elif(mot_tu_cuoi==beam_check[1][len(beam_check[1])-3]):\n","                      kolamgi=1\n","                    # elif(mot_tu_cuoi==beam_check[1][len(beam_check[1])-4]):\n","                    #   kolamgi=1\n","                    else:\n","                      list_accept_beam.append(beam_check)\n","                  # if(len(list_accept_beam)!=4):\n","                  #   print(list_accept_beam)\n","                  list_accept_beam.sort(key = lambda x:x[0], reverse = True)\n","                  beams = list_accept_beam[:beam_size] # truncate to get new beams \n","                  # beams = beam_candidates[:beam_size] # cat nhánh\n","                # print(beams)\n","                # print(beams)\n","            # print('beams',beams)\n","            print(\"input: \")\n","            print('start_sentence: '+list_input[0])\n","            print('path_to_ascoutics_panel: ',list_input[1])\n","            print('is beams: ',list_input[2])\n","            print('*****************************************************')\n","            if(beams!=[]):\n","              list_sentence_lyrics=(word_before_word_begin+' '+' '.join(beams[0][1][0:len(beams[0][1])])).split('.')\n","              if (sys.version_info > (3, 0)):\n","                  print(\"Lyrics generated: \")\n","                  print((word_before_word_begin+' '+' '.join(beams[0][1][0:len(beams[0][1])])).replace('.','\\n'))\n","                  print('*****************************************************')\n","                  print(\"Lyrics and chord generated base on ascoutics panel: \")\n","                  for yz in range(0,len(list_sentence_cam_am),1):\n","                    print(list_sentence_lyrics[yz])\n","                    # print(list_sentence_cam_am[yz])\n","                    print(list_sentence_cam_am_nhip[yz])\n","                  # with open('Result/'+str(get_number_file_result+1)+'.txt','a',encoding='utf-8') as file1:\n","                  #     file1.write((word_before_word_begin+' '+' '.join(beams[0][1][1:len(beams[0][1])])).replace('.','\\n'))\n","              else:\n","                  print(\"here\")\n","                  print('Generated Result:')\n","                  print(join(beams[0][1][1:len(beams[0][1])]))\n","                  # with open('Result/'+str(get_number_file_result+1)+'.txt','a',encoding='utf-8') as file1:\n","                  #     file1.write((' '.join(beams[0][1][1:len(beams[0][1])])))\n","\n","if __name__ == \"__main__\":\n","    tf.compat.v1.app.run()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Load_model: \n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-dd14b73dbd6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# input_=\"mùa đông mi_50_True.txt True\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mlist_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mlen_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m#khoi tạo input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'input_' is not defined"]}]},{"cell_type":"code","metadata":{"id":"I3R49rDmTrNB","colab":{"base_uri":"https://localhost:8080/","height":428},"executionInfo":{"status":"error","timestamp":1575910317800,"user_tz":-420,"elapsed":3313,"user":{"displayName":"Tuấn Tô Huỳnh","photoUrl":"","userId":"00317117433846939709"}},"outputId":"f5168cbd-c6d7-492d-c53b-ef488f2d7a41"},"source":["#coding:utf-8\n","import tensorflow as tf\n","import sys,time\n","import numpy as np\n","import pickle as cPickle\n","import os\n","import random\n","import Config\n","import Model\n","import codecs\n","from random import choice\n","config_tf = tf.compat.v1.ConfigProto()\n","config_tf.gpu_options.allow_growth = True\n","config_tf.inter_op_parallelism_threads = 1\n","config_tf.intra_op_parallelism_threads = 1\n","# get_number_file_result to set file name\n","# get_number_file_result=len(os.listdir('./Result/'))\n","# print(get_number_file_result)\n","config = Config.Config()\n","print(\"Load_model: \")\n","char_to_idx, idx_to_char = cPickle.load(open(config.model_path+'.voc', 'rb'))\n","\n","config.vocab_size = len(char_to_idx)\n","is_sample = config.is_sample\n","is_beams = False\n","beam_size = config.beam_size\n","# len_of_generation = config.len_of_generation\n","start_sentence =\"mưa 50\"\n","if (len(sys.argv) == 2):\n","    if (sys.version_info > (3, 0)):\n","        input_ = sys.argv[1]\n","    else:\n","        input_ = sys.argv[1].decode(\"utf-8\")\n","list_input=input_.split()\n","start_sentence=' '.join([list_input[0],list_input[1]])\n","is_beams=bool(list_input[2])\n","def run_epoch(session, m, data, eval_op, state=None):\n","    \"\"\"Runs the model on the given data.\"\"\"\n","    x = data.reshape((1,1))\n","    prob, _state, _ = session.run([m._prob, m.final_state, eval_op],\n","                         {m.input_data: x,\n","                          m.initial_state: state})\n","    return prob, _state\n","\n","def main(_):\n","    with tf.Graph().as_default(), tf.compat.v1.Session(config=config_tf) as session:\n","        config.batch_size = 1\n","        config.num_steps = 1\n","\n","        initializer = tf.random_uniform_initializer(-config.init_scale,\n","                                                config.init_scale)\n","        with tf.compat.v1.variable_scope(\"model\", reuse=None, initializer=initializer):\n","            mtest = Model.Model(is_training=False, config=config)\n","\n","        #tf.global_variables_initializer().run()\n","\n","        model_saver = tf.compat.v1.train.Saver()\n","        print(\"model loading ...\")\n","        model_saver.restore(session, config.model_path+'-%d'%config.save_time)\n","        # model_saver.restore(session, config.model_path+str(-5))\n","        print (\"Done!\")\n","        char_list = (start_sentence.split())\n","        word_before_word_begin=''\n","        if(len(char_list)>2):\n","            word_before_word_begin=' '.join(char_list[0:len(char_list)-2])\n","        # word=' '.join(char_list[0:len(char_list)-1])\n","        word=char_list[len(char_list)-2]\n","        len_of_generation=int(char_list[len(char_list)-1])\n","        if not is_beams:\n","            # sentence state\n","            # char_list = (start_sentence.split());\n","            # start_idx = char_to_idx[char_list[0]]\n","            start_idx = char_to_idx[word]\n","            _state = mtest.initial_state.eval()\n","            test_data = np.int32([start_idx])\n","            prob, _state = run_epoch(session, mtest, test_data, tf.no_op(), _state)\n","            # gen_res = [char_list[0]]\n","            gen_res = []\n","            # for i in range(1, len(char_list)):\n","            # for i in range(1, len(word)):\n","            char = word\n","            try:\n","                char_index = char_to_idx[char]\n","            except KeyError:\n","                char_index = np.argmax(prob.reshape(-1))\n","            prob, _state = run_epoch(session, mtest, np.int32([char_index]), tf.no_op(), _state)\n","            gen_res.append(char)\n","            # gen text\n","            if is_sample:\n","                gen = np.random.choice(config.vocab_size, 1, p=prob.reshape(-1))\n","                gen = gen[0]\n","            else:\n","                gen = np.argmax(prob.reshape(-1))\n","            test_data = np.int32(gen)\n","            gen_res.append(idx_to_char[gen])\n","            for i in range(len_of_generation-1):\n","                prob, _state = run_epoch(session, mtest, test_data, tf.no_op(), _state)\n","                if is_sample:\n","                    gen = np.random.choice(config.vocab_size, 1, p=prob.reshape(-1))\n","                    gen = gen[0]\n","                else:\n","                    gen = np.argmax(prob.reshape(-1))\n","                test_data = np.int32(gen)\n","                gen_res.append(idx_to_char[gen])\n","            # print(gen_res)\n","            print(\"Generated Result here:\")\n","            # print('ketqua: ')\n","            print((word_before_word_begin+(' ').join(gen_res)).replace('.','\\n'))\n","            # with open('Result/'+str(get_number_file_result+1)+'.txt','a',encoding='utf-8') as file1:\n","            #     file1.write((word_before_word_begin+(' ').join(gen_res)).replace('.','\\n'))\n","        else:\n","            # sentence state\n","            # char_list = (start_sentence).split();\n","            # start_idx = char_to_idx[char_list[0]]\n","            start_idx = char_to_idx[word]\n","            _state = mtest.initial_state.eval()\n","            beams = [(0.0, [idx_to_char[start_idx]], idx_to_char[start_idx])]\n","            test_data = np.int32([start_idx])\n","            prob, _state = run_epoch(session, mtest, test_data, tf.no_op(), _state)\n","            y1 = np.log(1e-20 + prob.reshape(-1))\n","            beams = [(beams[0][0], beams[0][1], beams[0][2], _state)]\n","            # for i in range(1, len(char_list)):\n","            # for i in range(1, len(word)):\n","            char = word\n","            print(\"char: \",char)\n","            try:\n","                char_index = char_to_idx[char]\n","            except KeyError:\n","                top_indices = np.argsort(-y1)\n","                char_index = top_indices[0]\n","            prob, _state = run_epoch(session, mtest, np.int32([char_index]), tf.no_op(), beams[0][3])\n","            y1 = np.log(1e-20 + prob.reshape(-1))\n","            beams = [(beams[0][0], beams[0][1] + [char], char_index, _state)]\n","            # gen text\n","            if is_sample:\n","                top_indices = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","            else:\n","                top_indices = np.argsort(-y1)\n","            b = beams[0]\n","            beam_candidates = []\n","            for i in range(beam_size):\n","                wordix = top_indices[i]\n","                beam_candidates.append((b[0] + y1[wordix], b[1] + [idx_to_char[wordix]], wordix, _state))\n","            beam_candidates.sort(key = lambda x:x[0], reverse = True) # decreasing order\n","            beams = beam_candidates[:beam_size] # truncate to get new beams\n","            for xy in range(len_of_generation-1):\n","                beam_candidates = []\n","                for b in beams:\n","                    test_data = np.int32(b[2])\n","                    prob, _state = run_epoch(session, mtest, test_data, tf.no_op(), b[3])\n","                    y1 = np.log(1e-20 + prob.reshape(-1))\n","                    if is_sample:\n","                        top_indices = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","                    else:\n","                        top_indices = np.argsort(-y1)\n","                    for i in range(beam_size):\n","                        wordix = top_indices[i]\n","                        beam_candidates.append((b[0] + y1[wordix], b[1] + [idx_to_char[wordix]], wordix, _state))\n","                beam_candidates.sort(key = lambda x:x[0], reverse = True) # decreasing order\n","                beams = beam_candidates[:beam_size] # truncate to get new beams\n","                # print(beams)\n","            if (sys.version_info > (3, 0)):\n","                print(\"Generated Result here2:\\n\")\n","                print((word_before_word_begin+' '+' '.join(beams[0][1][1:len(beams[0][1])])).replace('.','\\n'))\n","                # with open('Result/'+str(get_number_file_result+1)+'.txt','a',encoding='utf-8') as file1:\n","                #     file1.write((word_before_word_begin+' '+' '.join(beams[0][1][1:len(beams[0][1])])).replace('.','\\n'))\n","\n","if __name__ == \"__main__\":\n","    tf.compat.v1.app.run()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Load_model: \n","WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f338135d588>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"],"name":"stdout"},{"output_type":"stream","text":["W1209 16:51:53.343441 139860876388224 rnn_cell_impl.py:698] <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f338135d588>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"],"name":"stderr"},{"output_type":"stream","text":["model loading ...\n","INFO:tensorflow:Restoring parameters from ./model/Model-40\n"],"name":"stdout"},{"output_type":"stream","text":["I1209 16:51:53.490370 139860876388224 saver.py:1284] Restoring parameters from ./model/Model-40\n"],"name":"stderr"},{"output_type":"stream","text":["Done!\n","char:  anh\n","Generated Result here2:\n","\n"," anh yêu em nhiều hơn \n"," tìm về nơi 2199 đâu có em , anh đang ở bên em \n"," đến khi nào em mới biết anh yêu em trong 2157 đời \n"," anh sẽ 2128 nói em sẽ không bao giờ 2157 quên \n"," anh vẫn 2128 chờ , em vẫn\n"],"name":"stdout"},{"output_type":"error","ename":"SystemExit","evalue":"ignored","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\n"]},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"e-Y6TnPR34yY","colab":{"base_uri":"https://localhost:8080/","height":679},"executionInfo":{"status":"error","timestamp":1575483829170,"user_tz":-420,"elapsed":4293,"user":{"displayName":"Tuấn Tô Huỳnh","photoUrl":"","userId":"00317117433846939709"}},"outputId":"d30651ab-58ee-4693-e1f8-2bbe20fe7d21"},"source":["#coding:utf-8\n","#create cảm âm\n","import tensorflow as tf\n","import sys,time\n","import numpy as np\n","import pickle as cPickle\n","import os\n","import random\n","import Config\n","import Model\n","import codecs\n","from random import choice\n","config_tf = tf.compat.v1.ConfigProto()\n","config_tf.gpu_options.allow_growth = True\n","config_tf.inter_op_parallelism_threads = 1\n","config_tf.intra_op_parallelism_threads = 1\n","# get_number_file_result to set file name\n","# get_number_file_result=len(os.listdir('./Result/'))\n","# print(get_number_file_result)\n","config = Config.Config()\n","print(\"Load_model: \")\n","char_to_idx_cam_am, idx_to_char_cam_am = cPickle.load(open('./cam_am/model_cam_am/Model'+'.voc', 'rb'))\n","config.vocab_size = len(char_to_idx_cam_am)\n","config.model_path_cam_am='./cam_am/model_cam_am/Model'\n","is_sample = config.is_sample\n","beam_size = config.beam_size\n","# len_of_generation = config.len_of_generation\n","start_sentence = \"pha 100 True\"\n","# start_sentence=start_sentence len isbeam\n","#xử lý input\n","if (len(sys.argv) == 2):\n","    if (sys.version_info > (3, 0)):\n","        start_sentence = sys.argv[1]\n","    else:\n","        start_sentence = sys.argv[1].decode(\"utf-8\")\n","def process_start_sentence(start_sentence):\n","  list_word=start_sentence.split()\n","  len_=len(list_word)\n","  sentence=' '.join(list_word[:(len_-2)])\n","  len_lyric=list_word[len_-2]\n","  isbeams=list_word[len_-1]\n","  start_word=sentence.split()[len(sentence.split())-1]\n","  return sentence,start_word,int(len_lyric),bool(isbeams)\n","input=process_start_sentence(start_sentence)\n","def show_input(input):\n","  print('Input: ')\n","  print(' +result_begin_with: ',input[0])\n","  print(' +with_len: ',input[2])\n","  print(' +using Beams search: ',input[3])\n","  return 0\n","is_beams = process_start_sentence(start_sentence)[3]\n","char_list = (start_sentence.split())\n","word_before_word_begin=process_start_sentence(start_sentence)[0]\n","# if(len(char_list)>2):\n","#     word_before_word_begin=' '.join(char_list[0:len(char_list)-2])\n","# word=' '.join(char_list[0:len(char_list)-1])\n","word=process_start_sentence(start_sentence)[1]\n","len_of_generation=process_start_sentence(start_sentence)[2]\n","def run_epoch(session, m, data, eval_op, state=None):\n","    \"\"\"Runs the model on the given data.\"\"\"\n","    x = data.reshape((1,1))\n","    prob, _state, _ = session.run([m._prob, m.final_state, eval_op],\n","                         {m.input_data: x,\n","                          m.initial_state: state})\n","    return prob, _state\n","\n","def main(_):\n","    with tf.Graph().as_default(), tf.compat.v1.Session(config=config_tf) as session:\n","        config.batch_size = 1\n","        config.num_steps = 1\n","\n","        initializer = tf.random_uniform_initializer(-config.init_scale,\n","                                                config.init_scale)\n","        with tf.compat.v1.variable_scope(\"model\", reuse=None, initializer=initializer):\n","            mtest = Model.Model(is_training=False, config=config)\n","\n","        #tf.global_variables_initializer().run()\n","\n","        model_saver = tf.compat.v1.train.Saver()\n","        print(\"model cam_am loading ...\")\n","        # model_saver.restore(session, config.model_path_cam_am+'-%d'%config.save_time)\n","        model_saver.restore(session,'./cam_am/model_cam_am/Model'+'-%d'%config.save_time)\n","        # ./cam_am/model_cam_am/Model\n","        # model_saver.restore(session, config.model_path+str(-5))\n","        print (\"Model_cam_cam: Done!\")\n","        if not is_beams:\n","            # sentence state\n","            # char_list = (start_sentence.split());\n","            # start_idx = char_to_idx[char_list[0]]\n","            start_idx = char_to_idx_cam_am[word]\n","            _state = mtest.initial_state.eval()\n","            test_data = np.int32([start_idx])\n","            prob, _state = run_epoch(session, mtest, test_data, tf.no_op(), _state)\n","            # gen_res = [char_list[0]]\n","            gen_res = []\n","            # for i in range(1, len(char_list)):\n","            # for i in range(1, len(word)):\n","            char = word\n","            try:\n","                char_index = char_to_idx_cam_am[char]\n","            except KeyError:\n","                char_index = np.argmax(prob.reshape(-1))\n","            prob, _state = run_epoch(session, mtest, np.int32([char_index]), tf.no_op(), _state)\n","            gen_res.append(char)\n","            # gen text\n","            if is_sample:\n","                gen = np.random.choice(config.vocab_size, 1, p=prob.reshape(-1))\n","                gen = gen[0]\n","            else:\n","                gen = np.argmax(prob.reshape(-1))\n","            test_data = np.int32(gen)\n","            gen_res.append(idx_to_char[gen])\n","            for i in range(len_of_generation-1):\n","                prob, _state = run_epoch(session, mtest, test_data, tf.no_op(), _state)\n","                if is_sample:\n","                    gen = np.random.choice(config.vocab_size, 1, p=prob.reshape(-1))\n","                    gen = gen[0]\n","                else:\n","                    gen = np.argmax(prob.reshape(-1))\n","                test_data = np.int32(gen)\n","                gen_res.append(idx_to_char_cam_am[gen])\n","            print(\"Generated Result here:\")\n","            print((word_before_word_begin+(' ').join(gen_res)).replace('.','\\n'))\n","            # with open('Result/'+str(get_number_file_result+1)+'.txt','a',encoding='utf-8') as file1:\n","            #     file1.write((word_before_word_begin+(' ').join(gen_res)).replace('.','\\n'))\n","        else:\n","            # sentence state\n","            # char_list = (start_sentence).split();\n","            # start_idx = char_to_idx[char_list[0]]\n","            start_idx = char_to_idx_cam_am[word]\n","            _state = mtest.initial_state.eval()\n","            beams = [(0.0, [idx_to_char_cam_am[start_idx]], idx_to_char_cam_am[start_idx])]\n","            test_data = np.int32([start_idx])\n","            prob, _state = run_epoch(session, mtest, test_data, tf.no_op(), _state)\n","            y1 = np.log(1e-20 + prob.reshape(-1))\n","            beams = [(beams[0][0], beams[0][1], beams[0][2], _state)]\n","            char = word\n","            print(\"char: \",char)\n","            try:\n","                char_index = char_to_idx_cam_am[char]\n","            except KeyError:\n","                top_indices = np.argsort(-y1)\n","                char_index = top_indices[0]\n","            prob, _state = run_epoch(session, mtest, np.int32([char_index]), tf.no_op(), beams[0][3])\n","            y1 = np.log(1e-20 + prob.reshape(-1))\n","            beams = [(beams[0][0],beams[0][1], char_index, _state)]\n","            # gen text\n","            check_first_word=True\n","            while(check_first_word):\n","              if is_sample:\n","                  top_indices = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","              else:\n","                  top_indices = np.argsort(-y1)\n","              print(top_indices)\n","              hai_word=[idx_to_char_cam_am[top_indices[0]],idx_to_char_cam_am[top_indices[1]]]\n","              if('.' in hai_word):\n","                check_first_word=True\n","              else:\n","                check_first_word=False \n","            print(hai_word)\n","            b = beams[0]\n","            beam_candidates = []\n","            for i in range(beam_size):\n","                wordix = top_indices[i]\n","                beam_candidates.append((b[0] + y1[wordix], b[1] + [idx_to_char_cam_am[wordix]], wordix, _state))\n","            beam_candidates.sort(key = lambda x:x[0], reverse = True) # decreasing order\n","            print(beam_candidates)\n","            beams = beam_candidates[:beam_size] # truncate to get new beams\n","            for xy in range(len_of_generation-1):\n","                beam_candidates = []\n","                for b in beams:\n","                    test_data = np.int32(b[2])\n","                    prob, _state = run_epoch(session, mtest, test_data, tf.no_op(), b[3])\n","                    y1 = np.log(1e-20 + prob.reshape(-1))\n","                    if is_sample:\n","                        top_indices = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","                    else:\n","                        top_indices = np.argsort(-y1)\n","                    for i in range(beam_size):\n","                        wordix = top_indices[i]\n","                        beam_candidates.append((b[0] + y1[wordix], b[1] + [idx_to_char_cam_am[wordix]], wordix, _state))\n","                beam_candidates.sort(key = lambda x:x[0], reverse = True) # decreasing order\n","                # print(beam_candidates)\n","                list_word_a=[]\n","                list_priority1=[]\n","                list_priority_sentence=[]\n","                #ý tưởng: nhận thấy sự lặp từ diễn ra quá nhiều và không có phương hướng đề giải quyết\n","                #ưu tiên accept trong 4 từ những từ là . hoặc ,\n","                for word_a in beam_candidates:\n","                  full_sentence=(' '.join(word_a[1]))\n","                  last_sentence=(' '.join(word_a[1])).split('.')[len((' '.join(word_a[1])).split('.'))-1]\n","                  last_word=word_a[1][len(word_a[1])-1]\n","                  # print(full_sentence)\n","                  list_word_a.append(word_a[1][len(word_a[1])-1])\n","                  if(',' in last_word or last_word=='.'):\n","                    list_priority1.append(word_a)\n","                  if(',' in last_sentence):\n","                    list_priority_sentence.append(word_a)\n","                # print(\"4 word accept: \",list_word_a)\n","                # print(\"list_priority: \",list_priority1)\n","                #nếu danh sách ưu tiên khác null\n","                if(len(list_priority1)!=0):\n","                  #nếu danh sách có từ 2 thì chọn random 2 phần tử để xét tiếp\n","                  if(len(list_priority1)>=2):\n","                    beams=list_priority1[:beam_size]\n","                  else:\n","                    # ngược lại accept 1 phần tử ưu tiên và chọn random 1 phần tử để xét tiếp\n","                    list_conlai=[]\n","                    for _beams in beam_candidates:\n","                      if(_beams!=list_priority1[0]):\n","                        list_conlai.append(_beams)\n","                    # print(list_conlai[0])\n","                    # print(list_priority1)\n","                    beams=[list_priority1[0],list_conlai[0]]\n","                else:\n","                  # trong trường hợp không tồn tại từ ưu tiên trong 4 từ\n","                  #xét ưu tiên câu: ưu tiên câu chứa ,quá trình xét tương tự như xét ưu tiên từ\n","                  if(len(list_priority_sentence)!=0):\n","                    if(len(list_priority_sentence)>=2):\n","                      beams=list_priority_sentence[:beam_size]\n","                    else:\n","                      list_conlai_sentence=[]\n","                      for _beams in beam_candidates:\n","                        if(_beams!=list_priority_sentence[0]):\n","                          list_conlai_sentence.append(_beams)\n","                      print(list_conlai_sentence[0])\n","                      print(list_priority_sentence)\n","                      beams=[list_priority_sentence[0],list_conlai_sentence[0]]\n","                  else:\n","                    # ngược lại accept 2 phần từ như bình thường\n","                    beams = beam_candidates[:beam_size] # truncate to get new beams\n","                # print(\"beams\",beams)\n","                list_sentence_result=(word_before_word_begin+' '+' '.join(beams[0][1][1:len(beams[0][1])])).split('.')\n","            # print(\"list_sentence_result: \\n\",list_sentence_result)\n","            show_input(input)\n","            print(\"Generated Result here:\")\n","            print(len(beams[0][1][0:len(beams[0][1])]))\n","            path_to_file_cam_am='cam_am/Result_cam_am/'+word_before_word_begin+'_'+str(len_of_generation)+'_'+str(is_beams)+'.txt'\n","            # path_to_file_cam_am='cam_am/Result_cam_am_nhip/'+word_before_word_begin+'_'+str(len_of_generation)+'_'+str(is_beams)+'.txt'\n","            print(\"path_to_file_cam_am: \",path_to_file_cam_am)\n","            result=(word_before_word_begin+' '+' '.join(beams[0][1][1:len(beams[0][1])])).replace('.','\\n')\n","            result=result.replace(',,,',' ,')\n","            result=result.replace(',',' ,')\n","            print(result)\n","            # print(module.add_nhip(path_to_file_cam_am))\n","            with open(path_to_file_cam_am,'w',encoding='utf-8') as file1:\n","              file1.write(result)\n","            # if (sys.version_info > (3, 0)):\n","            #     print(\"Generated Result here2:\\n\")\n","            #     # print(beams[0][1][0:len(beams[0][1])])\n","            #     print(len(beams[0][1][0:len(beams[0][1])]))\n","            #     print((word_before_word_begin+' '+' '.join(beams[0][1][1:len(beams[0][1])])).replace('.','\\n'))\n","            #     # with open('Result/'+str(get_number_file_result+1)+'.txt','a',encoding='utf-8') as file1:\n","            #     #     file1.write((word_before_word_begin+' '+' '.join(beams[0][1][1:len(beams[0][1])])).replace('.','\\n'))\n","            # else:\n","            #     print(\"here\")\n","            #     print('Generated Result:')\n","            #     print(join(beams[0][1][1:len(beams[0][1])]))\n","            #     # with open('Result/'+str(get_number_file_result+1)+'.txt','a',encoding='utf-8') as file1:\n","            #     #     file1.write((' '.join(beams[0][1][1:len(beams[0][1])])))\n","\n","if __name__ == \"__main__\":\n","    tf.compat.v1.app.run()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Load_model: \n","WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f94f41f26d8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"],"name":"stdout"},{"output_type":"stream","text":["W1204 18:23:46.132525 140279877162880 rnn_cell_impl.py:698] <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f94f41f26d8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"],"name":"stderr"},{"output_type":"stream","text":["model cam_am loading ...\n","INFO:tensorflow:Restoring parameters from ./cam_am/model_cam_am/Model-40\n"],"name":"stdout"},{"output_type":"stream","text":["I1204 18:23:46.222183 140279877162880 saver.py:1284] Restoring parameters from ./cam_am/model_cam_am/Model-40\n"],"name":"stderr"},{"output_type":"stream","text":["Model_cam_cam: Done!\n","char:  pha\n","[171 107]\n","['pha,', 'la']\n","[(-0.16735398769378662, ['pha', 'la'], 107, array([[ 1.1067050e-02, -3.5241351e-02, -1.1617826e-01, ...,\n","         7.9400468e-01, -7.6541585e-01,  3.9787545e-05]], dtype=float32)), (-7.817580223083496, ['pha', 'pha,'], 171, array([[ 1.1067050e-02, -3.5241351e-02, -1.1617826e-01, ...,\n","         7.9400468e-01, -7.6541585e-01,  3.9787545e-05]], dtype=float32))]\n","Input: \n"," +result_begin_with:  pha\n"," +with_len:  100\n"," +using Beams search:  True\n","Generated Result here:\n","101\n","path_to_file_cam_am:  cam_am/Result_cam_am/pha_100_True.txt\n","pha pha , son son đô pha pha pha \n"," pha pha pha son son , son son son pha mi pha \n"," đô pha mi pha mi rê đô rê \n"," rê pha son la , son pha son rê , đô \n"," đô rê pha , pha pha pha pha son la \n"," rê mi pha mi , mi pha mi rê đô \n"," rê pha son son son son pha mi pha son la \n"," rê rê rê rê , rê đô đô , rê pha son la pha \n"," rê pha rê son son , son pha son la \n"," rê mi pha son , pha son\n"],"name":"stdout"},{"output_type":"error","ename":"SystemExit","evalue":"ignored","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\n"]},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"sIuc663nA15-","colab":{"base_uri":"https://localhost:8080/","height":436},"executionInfo":{"status":"error","timestamp":1575893466614,"user_tz":-420,"elapsed":7394,"user":{"displayName":"Tuấn Tô Huỳnh","photoUrl":"","userId":"00317117433846939709"}},"outputId":"4a632c50-7aaa-4fca-c713-c45ce5bd9af0"},"source":["#coding:utf-8\n","import tensorflow as tf\n","import sys,time\n","import numpy as np\n","import pickle as cPickle\n","import os\n","import random\n","import Config\n","import Model\n","import codecs\n","import module\n","from random import choice\n","config_tf = tf.compat.v1.ConfigProto()\n","config_tf.gpu_options.allow_growth = True\n","config_tf.inter_op_parallelism_threads = 1\n","config_tf.intra_op_parallelism_threads = 1\n","#set_up_cac_gia_tri:\n","list_word_before_dot=module.get_list_before_dot_or_phay('before_dot.csv')\n","list_word_before_phay=module.get_list_before_dot_or_phay('before_phay.csv')\n","list_2word_before_dot=module.get_list_before_dot_or_phay('file_config/before_dot2.csv')\n","list_2word_before_phay=module.get_list_before_dot_or_phay('file_config/before_phay2.csv')\n","list_dong_am=['mãi mãi','luôn luôn']\n","list_ba_tu_lap=['em yêu em','anh yêu anh','yêu em yêu','yêu anh yêu','em ơi em','anh ơi anh']\n","list_bon_tu_lap=[\"anh vẫn yêu anh\",'em vẫn yêu em','anh vẫn mong anh','em vẫn mong em','em đã yêu em',\n","                  \"anh rất yêu anh\",\"em rất yêu em\",'anh đã yêu anh','em thật nhiều em ','em sẽ yêu em',\n","                  'anh sẽ yêu anh','em chỉ cần em','anh chỉ cần anh']\n","config = Config.Config()\n","print(\"Load_model: \")\n","char_to_idx, idx_to_char = cPickle.load(open(config.model_path+'.voc', 'rb'))\n","\n","config.vocab_size = len(char_to_idx)\n","is_sample = config.is_sample\n","is_beams = config.is_beams\n","beam_size = config.beam_size\n","# len_of_generation = config.len_of_generation\n","start_sentence = 'mưa 100'\n","#doc bang cam am\n","filename='cam_am/Result_cam_am/mi_50_True.txt'\n","filename_nhip='cam_am/Result_cam_am_nhip/mi_50_True.txt'\n","def read_file(path):\n","  with open(path, encoding='utf-8') as f:\n","      data = f.read()\n","  return data\n","data=read_file(filename)+ '.'\n","data_nhip=read_file(filename_nhip)\n","# data=data.split('\\n')[0]+'\\n'+data.split('\\n')[1]+'\\n'+data.split('\\n')[2]\n","print(data)\n","list_sentence_cam_am=data.split('\\n')\n","list_sentence_cam_am_nhip=data_nhip.split('\\n')\n","def get_len(sentence):\n","  return sentence.split()\n","def dem_word(sentence):\n","  list_word=sentence.split()\n","  list_word_not_chord=[]\n","  for word in list_word:\n","    if(word[0] not in ['0','1','2','3','4','5','6','7']):\n","      list_word_not_chord=list_word_not_chord+[word]\n","  # print(\"dem word trong cau: \",sentence,' ketqua là: ',dem)\n","  return list_word_not_chord\n","list_cam_am=(data.replace('\\n','.')+' .').split()\n","len_list_cam_am=len(list_cam_am)\n","print(list_cam_am)\n","print(len_list_cam_am)\n","if (len(sys.argv) == 2):\n","    if (sys.version_info > (3, 0)):\n","        start_sentence = sys.argv[1]\n","    else:\n","        start_sentence = sys.argv[1].decode(\"utf-8\")\n","\n","def run_epoch(session, m, data, eval_op, state=None):\n","    \"\"\"Runs the model on the given data.\"\"\"\n","    x = data.reshape((1,1))\n","    prob, _state, _ = session.run([m._prob, m.final_state, eval_op],\n","                         {m.input_data: x,\n","                          m.initial_state: state})\n","    return prob, _state\n","\n","def main(_):\n","    with tf.Graph().as_default(), tf.compat.v1.Session(config=config_tf) as session:\n","        config.batch_size = 1\n","        config.num_steps = 1\n","\n","        initializer = tf.random_uniform_initializer(-config.init_scale,\n","                                                config.init_scale)\n","        with tf.compat.v1.variable_scope(\"model\", reuse=None, initializer=initializer):\n","            mtest = Model.Model(is_training=False, config=config)\n","\n","        #tf.global_variables_initializer().run()\n","\n","        model_saver = tf.compat.v1.train.Saver()\n","        print(\"model loading ...\")\n","        model_saver.restore(session, config.model_path+'-%d'%config.save_time)\n","        # model_saver.restore(session, config.model_path+str(-5))\n","        print (\"Done!\")\n","        char_list = (start_sentence.split())\n","        word_before_word_begin=''\n","        if(len(char_list)>2):\n","            word_before_word_begin=' '.join(char_list[0:len(char_list)-2])\n","        # word=' '.join(char_list[0:len(char_list)-1])\n","        word=char_list[len(char_list)-2]\n","        len_of_generation=len_list_cam_am\n","        if not is_beams:\n","            # sentence state\n","            # char_list = (start_sentence.split());\n","            # start_idx = char_to_idx[char_list[0]]\n","            start_idx = char_to_idx[word]\n","            _state = mtest.initial_state.eval()\n","            test_data = np.int32([start_idx])\n","            prob, _state = run_epoch(session, mtest, test_data, tf.no_op(), _state)\n","            # gen_res = [char_list[0]]\n","            gen_res = []\n","            # for i in range(1, len(char_list)):\n","            # for i in range(1, len(word)):\n","            char = word\n","            try:\n","                char_index = char_to_idx[char]\n","            except KeyError:\n","                char_index = np.argmax(prob.reshape(-1))\n","            prob, _state = run_epoch(session, mtest, np.int32([char_index]), tf.no_op(), _state)\n","            gen_res.append(char)\n","            # gen text\n","            if is_sample:\n","                gen = np.random.choice(config.vocab_size, 1, p=prob.reshape(-1))\n","                gen = gen[0]\n","            else:\n","                gen = np.argmax(prob.reshape(-1))\n","            test_data = np.int32(gen)\n","            gen_res.append(idx_to_char[gen])\n","            for i in range(len_of_generation-1):\n","                prob, _state = run_epoch(session, mtest, test_data, tf.no_op(), _state)\n","                if is_sample:\n","                    gen = np.random.choice(config.vocab_size, 1, p=prob.reshape(-1))\n","                    gen = gen[0]\n","                else:\n","                    gen = np.argmax(prob.reshape(-1))\n","                test_data = np.int32(gen)\n","                gen_res.append(idx_to_char[gen])\n","            # print(gen_res)\n","            print(\"Generated Result here:\")\n","            # print('ketqua: ')\n","            print((word_before_word_begin+(' ').join(gen_res)).replace('.','\\n'))\n","            with open('Result/'+str(get_number_file_result+1)+'.txt','a',encoding='utf-8') as file1:\n","                file1.write((word_before_word_begin+(' ').join(gen_res)).replace('.','\\n'))\n","        else:\n","            # sentence state\n","            # char_list = (start_sentence).split();\n","            # start_idx = char_to_idx[char_list[0]]\n","            start_idx = char_to_idx[word]\n","            _state = mtest.initial_state.eval()\n","            beams = [(0.0, [idx_to_char[start_idx]], idx_to_char[start_idx])]\n","            test_data = np.int32([start_idx])\n","            prob, _state = run_epoch(session, mtest, test_data, tf.no_op(), _state)\n","            y1 = np.log(1e-20 + prob.reshape(-1))\n","            beams = [(beams[0][0], beams[0][1], beams[0][2], _state)]\n","            # for i in range(1, len(char_list)):\n","            # for i in range(1, len(word)):\n","            char = word\n","            print(\"char: \",char)\n","            try:\n","                char_index = char_to_idx[char]\n","            except KeyError:\n","                top_indices = np.argsort(-y1)\n","                char_index = top_indices[0]\n","            prob, _state = run_epoch(session, mtest, np.int32([char_index]), tf.no_op(), beams[0][3])\n","            y1 = np.log(1e-20 + prob.reshape(-1))\n","            beams = [(beams[0][0], beams[0][1], char_index, _state)]\n","            # gen text\n","            if(list_cam_am[len(char_list)-1]!='.' or list_cam_am[len(char_list)-1]!=','):\n","              status=True\n","              chord1=[]\n","              chord2=[]\n","              while(status):\n","                if is_sample:\n","                    top_indices = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","                else:\n","                    top_indices = np.argsort(-y1)\n","                if(idx_to_char[top_indices[0]]=='.' or idx_to_char[top_indices[1]]=='.'):\n","                  not_process=1\n","                elif(idx_to_char[top_indices[0]]==',' or idx_to_char[top_indices[1]]==','):\n","                  not_process=1\n","                elif(idx_to_char[top_indices[0]][0] in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"] and idx_to_char[top_indices[1]][0] not in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"]):\n","                  chord1=chord1+[idx_to_char[top_indices[0]]]\n","                  prob, _state = run_epoch(session, mtest, top_indices[0], tf.no_op(), beams[0][3])\n","                  top_indices1 = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","                  for top in top_indices1:\n","                    if(idx_to_char[top][0] not in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"]):\n","                      top_indices[0]=top\n","                  # print(top_indices1)\n","                  # print(idx_to_char[top_indices[0]],idx_to_char[top_indices[1]])\n","                elif(idx_to_char[top_indices[1]][0] in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"] and idx_to_char[top_indices[0]][0] not in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"]):\n","                  chord2=chord2+[idx_to_char[top_indices[1]]]\n","                  prob, _state = run_epoch(session, mtest, top_indices[1], tf.no_op(),beams[0][3])\n","                  top_indices2 = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","                  for top in top_indices2:\n","                    if(idx_to_char[top][0] not in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"]):\n","                      top_indices[1]=top\n","                  # print(top_indices2)\n","                elif(idx_to_char[top_indices[1]][0] in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"] and idx_to_char[top_indices[0]][0] in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"]):\n","                  chord1=chord1+[idx_to_char[top_indices[0]]]\n","                  chord2=chord2+[idx_to_char[top_indices[1]]]\n","                  prob, _state = run_epoch(session, mtest, test_data, tf.no_op(),beams[0][3])\n","                  check=True\n","                  while(check):\n","                    top_indices2 = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","                    # print(idx_to_char[top_indices2[0]],idx_to_char[top_indices2[1]])\n","                    if(idx_to_char[top_indices2[0]]==',' or idx_to_char[top_indices2[1]]==',' or idx_to_char[top_indices2[0]]=='.' or idx_to_char[top_indices2[1]]=='.'):\n","                      not_accept=1\n","                    elif(idx_to_char[top_indices2[0]][0] not in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"] and idx_to_char[top_indices2[1]][0] not in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"]):\n","                      top_indices=top_indices2\n","                      check=False\n","                      status=False\n","                else:\n","                  status=False\n","            else:\n","              if is_sample:\n","                    top_indices = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","              else:\n","                    top_indices = np.argsort(-y1)\n","            b = beams[0]\n","            beam_candidates = []\n","            for i in range(beam_size):\n","                wordix = top_indices[i]\n","                beam_candidates.append((b[0] + y1[wordix], b[1] + [idx_to_char[wordix]], wordix, _state))\n","            beam_candidates.sort(key = lambda x:x[0], reverse = True) # decreasing order\n","            beams = beam_candidates[:beam_size] # truncate to get new beams\n","            print(beams)\n","            print(list_cam_am[1])\n","            #chạy từ 2 vì word_begin+first_word_using_for_create_beam\n","            for xy in range(2,len_of_generation-1,1):\n","                beam_candidates = []\n","                # print(list_cam_am[xy])\n","                if(list_cam_am[xy]=='.'):\n","                  # print('in here')\n","                  for b in beams:\n","                      list_sentence_dot=' '.join(b[1]).split('.') #danh sach cau dang xét\n","                      len_list_sentence_dot=len(list_sentence_dot) #len của danh sach đang xét\n","                      last_sentence_dot=list_sentence_dot[len_list_sentence_dot-1] #câu cuối cùng trong danh sách\n","                      last_sentence_cam_am=list_sentence_cam_am[len_list_sentence_dot-1] #câu cuối cảm âm đang xét\n","                      len_last_senetence_dot=(len(dem_word(last_sentence_dot))) # chiều dài câu đang xét\n","                      len_last_sentence_cam_am=len(last_sentence_cam_am.split()) # chiều dài câu cảm âm cuối\n","                      last_word_before_dot=dem_word(last_sentence_dot)[len_last_senetence_dot-1] # từ cuối cùng trong danh sách\n","                      print('last_sentence_dot',last_sentence_dot)\n","                      # print('last_word_before_dot',last_word_before_dot)\n","                      # print(last_sentence_cam_am)\n","                      list_word=[]\n","                      # if(len_last_sentence_cam_am-len_last_senetence_dot==2): \n","                      len_last_senetence_dot_original=len_last_senetence_dot\n","                      while(len_last_sentence_cam_am!=len_last_senetence_dot):\n","                        print(len_last_sentence_cam_am,len_last_senetence_dot)\n","                        test_data = np.int32(b[2])\n","                        prob, _state = run_epoch(session, mtest, test_data, tf.no_op(), b[3])\n","                        y1 = np.log(1e-20 + prob.reshape(-1))\n","                        so_word_thieu=len_last_sentence_cam_am-len_last_senetence_dot\n","                        print(so_word_thieu)\n","                        # while(dem_word!=0)\n","                        top_indices = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","                        # print([idx_to_char[top_indices[0]],idx_to_char[top_indices[1]]])\n","                        for first_character in [idx_to_char[top_indices[0]],idx_to_char[top_indices[1]]]:\n","                          if(len(first_character.split())==2):\n","                            if(first_character!='.' and first_character!=',' and first_character[0] not in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"] and last_word_before_dot+' '+first_character in list_2word_before_dot):\n","                              list_word=list_word+[first_character]\n","                              len_last_senetence_dot+=len(first_character.split())\n","                              if(len_last_sentence_cam_am==len_last_senetence_dot):\n","                                break\n","                          elif(len(first_character.split())==1):\n","                            if(first_character!='.' and first_character!=',' and first_character[0] not in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"] and last_word_before_dot+' '+first_character in list_2word_before_dot):\n","                              list_word=list_word+[first_character]\n","                              len_last_senetence_dot+=len(first_character.split())\n","                              if(len_last_sentence_cam_am==len_last_senetence_dot):\n","                                break\n","                          else:\n","                            boqua=1\n","                        if(len_last_senetence_dot>len_last_sentence_cam_am):\n","                          print('in here: ')\n","                          print(len_last_sentence_cam_am,len_last_senetence_dot)\n","                          len_last_sentence_dot=len_last_senetence_dot_original\n","                          # else:\n","\n","                      print(\"list_word_dot: \",list_word)\n","                      test_data = np.int32(b[2])\n","                      prob, _state = run_epoch(session, mtest, test_data, tf.no_op(), b[3])\n","                      y1 = np.log(1e-20 + prob.reshape(-1))\n","                      if is_sample:\n","                            top_indices = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","                      else:\n","                          top_indices = np.argsort(-y1)\n","                      if(len(list_word)!=0):\n","                        if(idx_to_char[top_indices[0]]=='.' or idx_to_char[top_indices[1]]=='.'):\n","                          for i in range(beam_size):\n","                            wordix = top_indices[i]\n","                            beam_candidates.append((b[0] + y1[wordix] + y1[char_to_idx[list_word[0]]], b[1] + [list_word[0]] + [idx_to_char[wordix]], char_to_idx['.'], _state))\n","                        else:\n","                          for i in range(beam_size):\n","                            wordix = char_to_idx['.']\n","                            beam_candidates.append((b[0] + y1[wordix] + y1[char_to_idx[list_word[0]]], b[1]+ [list_word[0]] + [idx_to_char[wordix]], wordix, _state))\n","                      else:\n","                        if(idx_to_char[top_indices[0]]=='.' or idx_to_char[top_indices[1]]=='.'):\n","                          for i in range(beam_size):\n","                            wordix = top_indices[i]\n","                            beam_candidates.append((b[0] + y1[wordix], b[1] + [idx_to_char[wordix]], char_to_idx['.'], _state))\n","                        else:\n","                          for i in range(beam_size):\n","                            wordix = char_to_idx['.']\n","                            beam_candidates.append((b[0] + y1[wordix], b[1] + [idx_to_char[wordix]], wordix, _state))\n","                elif(list_cam_am[xy]==','):\n","                  for b in beams:\n","                      list_sentence_dot=' '.join(b[1]).split('.')\n","                      # print(list_sentence_dot)\n","                      len_list_sentence_dot=len(list_sentence_dot)\n","                      last_sentence_before_phay=list_sentence_dot[len_list_sentence_dot-1]\n","                      last_sentence_cam_am=list_sentence_cam_am[len_list_sentence_dot-1]\n","                      sentence_before_phay=last_sentence_cam_am.split(',')[0]\n","                      len_last_sentence_before_phay=len(dem_word(last_sentence_before_phay))\n","                      len_sentence_cam_am_before_phay=len(sentence_before_phay.split())\n","                      last_word_before_phay=dem_word(last_sentence_before_phay)[len_last_sentence_before_phay-1]\n","                      # print(last_sentence_before_phay)\n","                      # print(\"len_last_sentence: \",len_last_sentence_before_phay)\n","                      # print(sentence_before_phay)\n","                      # print(\"len_cam_am: \",len_sentence_cam_am_before_phay)\n","                      list_word=[]\n","                      len_last_sentence_before_phay_original=len_last_sentence_before_phay\n","                      while(len_last_sentence_before_phay!=len_sentence_cam_am_before_phay):\n","                        print(len_last_sentence_before_phay,len_sentence_cam_am_before_phay)\n","                        test_data = np.int32(b[2])\n","                        prob, _state = run_epoch(session, mtest, test_data, tf.no_op(), b[3])\n","                        y1 = np.log(1e-20 + prob.reshape(-1))\n","                        top_indices = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","                        print(\"len trong phay: \",len_last_sentence_before_phay,len_sentence_cam_am_before_phay)\n","                        print(\"list trong phay: \",[idx_to_char[top_indices[0]],idx_to_char[top_indices[1]]])\n","                        print('last_sentence_before_phay: ',last_sentence_before_phay)\n","                        for first_character in [idx_to_char[top_indices[0]],idx_to_char[top_indices[1]]]:\n","                          two_word=last_word_before_phay+' '+first_character\n","                          if(len(first_character.split())==2):\n","                            if(first_character!='.' and first_character!=',' and first_character[0] not in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"] and  two_word in list_2word_before_phay):\n","                              list_word=list_word+[first_character]\n","                              len_last_sentence_before_phay+=len(first_character.split())\n","                              if(len_last_sentence_before_phay==len_sentence_cam_am_before_phay):\n","                                break\n","                          elif(len(first_character.split())==1):\n","                            if(first_character!='.' and first_character!=',' and first_character[0] not in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"] and two_word in list_2word_before_phay):\n","                              list_word=list_word+[first_character]\n","                              len_last_sentence_before_phay+=len(first_character.split())\n","                              if(len_last_sentence_before_phay==len_sentence_cam_am_before_phay):\n","                                break\n","                          else:\n","                            boqua=1\n","                        if(len_last_sentence_before_phay>len_sentence_cam_am_before_phay):\n","                          len_last_sentence_before_phay=len_last_sentence_before_phay_original\n","                      print(list_word)\n","                      test_data = np.int32(b[2])\n","                      prob, _state = run_epoch(session, mtest, test_data, tf.no_op(), b[3])\n","                      y1 = np.log(1e-20 + prob.reshape(-1))\n","                      if is_sample:\n","                            top_indices = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","                      else:\n","                          top_indices = np.argsort(-y1)\n","                      if(len(list_word)!=0):\n","                        if(idx_to_char[top_indices[0]]==',' or idx_to_char[top_indices[1]]==','):\n","                          for i in range(beam_size):\n","                            wordix = top_indices[i]\n","                            beam_candidates.append((b[0] + y1[wordix], b[1] + [list_word[0]] + [idx_to_char[wordix]], wordix, _state))\n","                        else:\n","                          for i in range(beam_size):\n","                            wordix = char_to_idx[',']\n","                            beam_candidates.append((b[0] + y1[wordix], b[1]+ [list_word[0]] + [idx_to_char[wordix]], wordix, _state))\n","                      else:\n","                        if(idx_to_char[top_indices[0]]==',' or idx_to_char[top_indices[1]]==','):\n","                          for i in range(beam_size):\n","                            wordix = top_indices[i]\n","                            beam_candidates.append((b[0] + y1[wordix], b[1] + [idx_to_char[wordix]], wordix, _state))\n","                        else:\n","                          for i in range(beam_size):\n","                            wordix = char_to_idx[',']\n","                            beam_candidates.append((b[0] + y1[wordix], b[1] + [idx_to_char[wordix]], wordix, _state))\n","                else:\n","                  for b in beams:\n","                      test_data = np.int32(b[2])\n","                      prob, _state = run_epoch(session, mtest, test_data, tf.no_op(), b[3])\n","                      y1 = np.log(1e-20 + prob.reshape(-1))\n","                      status=True \n","                      chord1=[]\n","                      chord2=[]\n","                      while(status):\n","                        if is_sample:\n","                            top_indices = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","                        else:\n","                            top_indices = np.argsort(-y1)\n","                        if(idx_to_char[top_indices[0]]=='.' or idx_to_char[top_indices[1]]=='.'):\n","                          not_accept=1\n","                        elif(idx_to_char[top_indices[0]]==',' or idx_to_char[top_indices[1]]==','):\n","                          not_accept=1\n","                        # elif(idx_to_char[top_indices[0]][0] in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"] and idx_to_char[top_indices[1]][0] not in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"]):\n","                        #   chord1=chord1+[idx_to_char[top_indices[0]]]\n","                        #   prob, _state = run_epoch(session, mtest, top_indices[0], tf.no_op(), b[3])\n","                        #   top_indices1 = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","                        #   for top in top_indices1:\n","                        #     if(idx_to_char[top][0] not in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"]):\n","                        #       top_indices[0]=top\n","                        #   # print(top_indices1)\n","                        #   # print(idx_to_char[top_indices[0]],idx_to_char[top_indices[1]])\n","                        # elif(idx_to_char[top_indices[1]][0] in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"] and idx_to_char[top_indices[0]][0] not in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"]):\n","                        #   chord2=chord2+[idx_to_char[top_indices[1]]]\n","                        #   prob, _state = run_epoch(session, mtest, top_indices[1], tf.no_op(), b[3])\n","                        #   top_indices2 = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","                        #   for top in top_indices2:\n","                        #     if(idx_to_char[top][0] not in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"]):\n","                        #       top_indices[1]=top\n","                        #   # print(top_indices2)\n","                        # elif(idx_to_char[top_indices[1]][0] in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"] and idx_to_char[top_indices[0]][0] in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"]):\n","                        #   chord1=chord1+[idx_to_char[top_indices[0]]]\n","                        #   chord2=chord2+[idx_to_char[top_indices[1]]]\n","                        #   prob, _state = run_epoch(session, mtest, test_data, tf.no_op(), b[3])\n","                        #   check=True\n","                        #   while(check):\n","                        #     top_indices2 = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","                        #     # print(idx_to_char[top_indices2[0]],idx_to_char[top_indices2[1]])\n","                        #     if(idx_to_char[top_indices2[0]]==',' or idx_to_char[top_indices2[1]]==',' or idx_to_char[top_indices2[0]]=='.' or idx_to_char[top_indices2[1]]=='.'):\n","                        #       not_accept=1\n","                        #     elif(idx_to_char[top_indices2[0]][0] not in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"] and idx_to_char[top_indices2[1]][0] not in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"]):\n","                        #       top_indices=top_indices2\n","                        #       check=False\n","                        #       status=False\n","                        #   # print(top_indices2)\n","                        else:\n","                          status=False\n","                      # print(idx_to_char[top_indices[0]],idx_to_char[top_indices[1]])\n","                      list_chord=[chord1,chord2]\n","                      # print(list_chord)\n","                      for i in range(beam_size):\n","                          wordix = top_indices[i]\n","                          beam_candidates.append((b[0] + y1[wordix], b[1] + list_chord[i]+[idx_to_char[wordix]], wordix, _state))\n","                beam_candidates.sort(key = lambda x:x[0], reverse = True) # decreasing order\n","                print(beam_candidates)\n","                if(list_cam_am[xy]=='.'):\n","                  list_accept_beam=[]\n","                  for beam_check in beam_candidates:\n","                    list_not_chord=module.get_list_not_chord(beam_check[1])\n","                    print(list_not_chord)\n","                    mot_tu_cuoi=module.get_last_string(list_not_chord,1)\n","                    hai_tu_cuoi=module.get_last_string(list_not_chord,2)\n","                    ba_tu_cuoi=module.get_last_string(list_not_chord,3)\n","                    mot_tu_before_dot=hai_tu_cuoi.split()[0]\n","                    hai_tu_before_dot=ba_tu_cuoi.split()[0]+' '+ba_tu_cuoi.split()[1] \n","                    if(mot_tu_cuoi=='.' and hai_tu_before_dot in list_2word_before_dot):\n","                      list_accept_beam.append(beam_check)\n","                    elif(mot_tu_cuoi=='.' and mot_tu_before_dot in list_word_before_dot and hai_tu_before_dot not in list_2word_before_dot):\n","                      list_accept_beam.append(beam_check)\n","                  if(len(list_accept_beam)!=4):\n","                    print('list_accpet_beam',list_accept_beam)\n","                  list_accept_beam.sort(key = lambda x:x[0], reverse = True)\n","                  beams=list_accept_beam[:beam_size]\n","                elif(list_cam_am[xy]==','):\n","                  list_accept_beam=[]\n","                  for beam_check in beam_candidates:\n","                    list_not_chord=module.get_list_not_chord(beam_check[1])\n","                    mot_tu_cuoi=module.get_last_string(list_not_chord,1)\n","                    hai_tu_cuoi=module.get_last_string(list_not_chord,2)\n","                    ba_tu_cuoi=module.get_last_string(list_not_chord,3)\n","                    mot_tu_before_phay=hai_tu_cuoi.split()[0]\n","                    hai_tu_before_phay=ba_tu_cuoi.split()[0]+' '+ba_tu_cuoi.split()[1] \n","                    if(mot_tu_cuoi==',' and hai_tu_before_phay in list_word_before_phay):\n","                      list_accept_beam.append(beam_check)\n","                    elif(mot_tu_cuoi==',' and mot_tu_before_phay in list_word_before_phay and hai_tu_before_phay not in list_word_before_phay):\n","                      list_accept_beam.append(beam_check)\n","                  list_accept_beam.sort(key = lambda x:x[0], reverse = True)\n","                  beams=list_accept_beam[:beam_size]\n","                else:\n","                  # hay bo từ khi từ 4 == từ 1 nhỉ?\n","                  list_accept_beam=[]\n","                  for beam_check in beam_candidates:\n","                    list_not_chord=module.get_list_not_chord(beam_check[1])\n","                    mot_tu_cuoi=module.get_last_string(list_not_chord,1)\n","                    hai_tu_cuoi=module.get_last_string(list_not_chord,2)\n","                    # print(hai_tu_cuoi)\n","                    ba_tu_cuoi=module.get_last_string(list_not_chord,3)\n","                    bon_tu_cuoi=module.get_last_string(list_not_chord,4)\n","                    # print(ba_tu_cuoi)\n","                    if(mot_tu_cuoi=='.'):\n","                      kolamgi=1\n","                    elif(hai_tu_cuoi not in list_dong_am and hai_tu_cuoi.split()[0]==hai_tu_cuoi.split()[1] or hai_tu_cuoi=='anh em' or hai_tu_cuoi=='em anh'):\n","                      kolamgi=1\n","                    elif(ba_tu_cuoi in list_ba_tu_lap):\n","                      kolamgi=1\n","                    elif(bon_tu_cuoi in list_bon_tu_lap):\n","                      kolamgi=1\n","                    # test idea xet theo tu cuoi,tu cách cuoi 1 dv\n","                    elif(mot_tu_cuoi==beam_check[1][len(beam_check[1])-3]):\n","                      kolamgi=1\n","                    # elif(mot_tu_cuoi==beam_check[1][len(beam_check[1])-4]):\n","                    #   kolamgi=1\n","                    else:\n","                      list_accept_beam.append(beam_check)\n","                  # if(len(list_accept_beam)!=4):\n","                  #   print(list_accept_beam)\n","                  list_accept_beam.sort(key = lambda x:x[0], reverse = True)\n","                  beams = list_accept_beam[:beam_size] # truncate to get new beams \n","                  # beams = beam_candidates[:beam_size] # cat nhánh\n","                # print(beams)\n","                # print(beams)\n","            print('beams',beams)\n","            if(beams!=[]):\n","              list_sentence_lyrics=(word_before_word_begin+' '+' '.join(beams[0][1][0:len(beams[0][1])])).split('.')\n","              if (sys.version_info > (3, 0)):\n","                  print(\"Generated Result here2:\\n\")\n","                  print((word_before_word_begin+' '+' '.join(beams[0][1][0:len(beams[0][1])])).replace('.','\\n'))\n","                  print('\\n')\n","                  for yz in range(0,len(list_sentence_cam_am),1):\n","                    print(list_sentence_lyrics[yz])\n","                    print(list_sentence_cam_am[yz])\n","                    print(list_sentence_cam_am_nhip[yz])\n","                  # with open('Result/'+str(get_number_file_result+1)+'.txt','a',encoding='utf-8') as file1:\n","                  #     file1.write((word_before_word_begin+' '+' '.join(beams[0][1][1:len(beams[0][1])])).replace('.','\\n'))\n","              else:\n","                  print(\"here\")\n","                  print('Generated Result:')\n","                  print(join(beams[0][1][1:len(beams[0][1])]))\n","                  # with open('Result/'+str(get_number_file_result+1)+'.txt','a',encoding='utf-8') as file1:\n","                  #     file1.write((' '.join(beams[0][1][1:len(beams[0][1])])))\n","\n","if __name__ == \"__main__\":\n","    tf.compat.v1.app.run()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-758d921c2e79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Config'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"4cFD7sbbezLV","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1573918079621,"user_tz":-420,"elapsed":2847,"user":{"displayName":"Tuấn Tô Huỳnh","photoUrl":"","userId":"00317117433846939709"}},"outputId":"72e016c6-a1e1-4ec5-b032-5875273476c8"},"source":["#coding:utf-8\n","import tensorflow as tf\n","import sys,time\n","import numpy as np\n","import pickle as cPickle\n","import os\n","import random\n","import Config\n","import Model\n","import codecs\n","import module\n","from random import choice\n","config_tf = tf.compat.v1.ConfigProto()\n","config_tf.gpu_options.allow_growth = True\n","config_tf.inter_op_parallelism_threads = 1\n","config_tf.intra_op_parallelism_threads = 1\n","#set_up_cac_gia_tri:\n","list_word_before_dot=module.get_list_before_dot_or_phay('before_dot.csv')\n","list_word_before_phay=module.get_list_before_dot_or_phay('before_phay.csv')\n","list_dong_am=['mãi mãi','luôn luôn']\n","list_ba_tu_lap=['em yêu em','anh yêu anh','yêu em yêu','yêu anh yêu','em ơi em','anh ơi anh']\n","list_bon_tu_lap=[\"anh vẫn yêu anh\",'em vẫn yêu em','anh vẫn mong anh','em vẫn mong em','em đã yêu em',\n","                  \"anh rất yêu anh\",\"em rất yêu em\",'anh đã yêu anh','em thật nhiều em ','em sẽ yêu em',\n","                  'anh sẽ yêu anh','em chỉ cần em','anh chỉ cần anh']\n","config = Config.Config()\n","print(\"Load_model: \")\n","char_to_idx, idx_to_char = cPickle.load(open(config.model_path+'.voc', 'rb'))\n","\n","config.vocab_size = len(char_to_idx)\n","is_sample = config.is_sample\n","is_beams = config.is_beams\n","beam_size = config.beam_size\n","# len_of_generation = config.len_of_generation\n","start_sentence = 'mưa 100'\n","#doc bang cam am\n","filename='Result_cam_am/mi_100_True.txt'\n","with open(filename, encoding='utf-8') as f:\n","        data = f.read()\n","data=data.split('\\n')[0]+'\\n'+data.split('\\n')[1]+'\\n'+data.split('\\n')[2]\n","print(data)\n","list_sentence_cam_am=data.split('\\n')\n","list_cam_am=(data.replace('\\n','.')+' .').split()\n","len_list_cam_am=len(list_cam_am)\n","print(list_cam_am)\n","print(len_list_cam_am)\n","if (len(sys.argv) == 2):\n","    if (sys.version_info > (3, 0)):\n","        start_sentence = sys.argv[1]\n","    else:\n","        start_sentence = sys.argv[1].decode(\"utf-8\")\n","\n","def run_epoch(session, m, data, eval_op, state=None):\n","    \"\"\"Runs the model on the given data.\"\"\"\n","    x = data.reshape((1,1))\n","    prob, _state, _ = session.run([m._prob, m.final_state, eval_op],\n","                         {m.input_data: x,\n","                          m.initial_state: state})\n","    return prob, _state\n","\n","def main(_):\n","    with tf.Graph().as_default(), tf.compat.v1.Session(config=config_tf) as session:\n","        config.batch_size = 1\n","        config.num_steps = 1\n","\n","        initializer = tf.random_uniform_initializer(-config.init_scale,\n","                                                config.init_scale)\n","        with tf.compat.v1.variable_scope(\"model\", reuse=None, initializer=initializer):\n","            mtest = Model.Model(is_training=False, config=config)\n","\n","        #tf.global_variables_initializer().run()\n","\n","        model_saver = tf.compat.v1.train.Saver()\n","        print(\"model loading ...\")\n","        model_saver.restore(session, config.model_path+'-%d'%config.save_time)\n","        # model_saver.restore(session, config.model_path+str(-5))\n","        print (\"Done!\")\n","        char_list = (start_sentence.split())\n","        word_before_word_begin=''\n","        if(len(char_list)>2):\n","            word_before_word_begin=' '.join(char_list[0:len(char_list)-2])\n","        # word=' '.join(char_list[0:len(char_list)-1])\n","        word=char_list[len(char_list)-2]\n","        len_of_generation=len_list_cam_am\n","        if not is_beams:\n","            # sentence state\n","            # char_list = (start_sentence.split());\n","            # start_idx = char_to_idx[char_list[0]]\n","            start_idx = char_to_idx[word]\n","            _state = mtest.initial_state.eval()\n","            test_data = np.int32([start_idx])\n","            prob, _state = run_epoch(session, mtest, test_data, tf.no_op(), _state)\n","            # gen_res = [char_list[0]]\n","            gen_res = []\n","            # for i in range(1, len(char_list)):\n","            # for i in range(1, len(word)):\n","            char = word\n","            try:\n","                char_index = char_to_idx[char]\n","            except KeyError:\n","                char_index = np.argmax(prob.reshape(-1))\n","            prob, _state = run_epoch(session, mtest, np.int32([char_index]), tf.no_op(), _state)\n","            gen_res.append(char)\n","            # gen text\n","            if is_sample:\n","                gen = np.random.choice(config.vocab_size, 1, p=prob.reshape(-1))\n","                gen = gen[0]\n","            else:\n","                gen = np.argmax(prob.reshape(-1))\n","            test_data = np.int32(gen)\n","            gen_res.append(idx_to_char[gen])\n","            for i in range(len_of_generation-1):\n","                prob, _state = run_epoch(session, mtest, test_data, tf.no_op(), _state)\n","                if is_sample:\n","                    gen = np.random.choice(config.vocab_size, 1, p=prob.reshape(-1))\n","                    gen = gen[0]\n","                else:\n","                    gen = np.argmax(prob.reshape(-1))\n","                test_data = np.int32(gen)\n","                gen_res.append(idx_to_char[gen])\n","            # print(gen_res)\n","            print(\"Generated Result here:\")\n","            # print('ketqua: ')\n","            print((word_before_word_begin+(' ').join(gen_res)).replace('.','\\n'))\n","            with open('Result/'+str(get_number_file_result+1)+'.txt','a',encoding='utf-8') as file1:\n","                file1.write((word_before_word_begin+(' ').join(gen_res)).replace('.','\\n'))\n","        else:\n","            # sentence state\n","            # char_list = (start_sentence).split();\n","            # start_idx = char_to_idx[char_list[0]]\n","            start_idx = char_to_idx[word]\n","            _state = mtest.initial_state.eval()\n","            beams = [(0.0, [idx_to_char[start_idx]], idx_to_char[start_idx])]\n","            test_data = np.int32([start_idx])\n","            prob, _state = run_epoch(session, mtest, test_data, tf.no_op(), _state)\n","            y1 = np.log(1e-20 + prob.reshape(-1))\n","            beams = [(beams[0][0], beams[0][1], beams[0][2], _state)]\n","            # for i in range(1, len(char_list)):\n","            # for i in range(1, len(word)):\n","            char = word\n","            print(\"char: \",char)\n","            try:\n","                char_index = char_to_idx[char]\n","            except KeyError:\n","                top_indices = np.argsort(-y1)\n","                char_index = top_indices[0]\n","            prob, _state = run_epoch(session, mtest, np.int32([char_index]), tf.no_op(), beams[0][3])\n","            y1 = np.log(1e-20 + prob.reshape(-1))\n","            beams = [(beams[0][0], beams[0][1], char_index, _state)]\n","            # gen text\n","            # if(list_cam_am[len(char_list)-1]!='.' or list_cam_am[len(char_list)-1]!=','):\n","            #   status=True\n","            #   chord1=[]\n","            #   chord2=[]\n","            #   while(status):\n","            #     if is_sample:\n","            #         top_indices = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","            #     else:\n","            #         top_indices = np.argsort(-y1)\n","            #     if(idx_to_char[top_indices[0]]=='.' or idx_to_char[top_indices[1]]=='.'):\n","            #       not_process=1\n","            #     elif(idx_to_char[top_indices[0]]==',' or idx_to_char[top_indices[1]]==','):\n","            #       not_process=1\n","            #     elif(idx_to_char[top_indices[0]][0] in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"] and idx_to_char[top_indices[1]][0] not in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"]):\n","            #       chord1=chord1+[idx_to_char[top_indices[0]]]\n","            #       prob, _state = run_epoch(session, mtest, top_indices[0], tf.no_op(), beams[0][3])\n","            #       top_indices1 = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","            #       for top in top_indices1:\n","            #         if(idx_to_char[top][0] not in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"]):\n","            #           top_indices[0]=top\n","            #       # print(top_indices1)\n","            #       # print(idx_to_char[top_indices[0]],idx_to_char[top_indices[1]])\n","            #     elif(idx_to_char[top_indices[1]][0] in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"] and idx_to_char[top_indices[0]][0] not in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"]):\n","            #       chord2=chord2+[idx_to_char[top_indices[1]]]\n","            #       prob, _state = run_epoch(session, mtest, top_indices[1], tf.no_op(),beams[0][3])\n","            #       top_indices2 = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","            #       for top in top_indices2:\n","            #         if(idx_to_char[top][0] not in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"]):\n","            #           top_indices[1]=top\n","            #       # print(top_indices2)\n","            #     elif(idx_to_char[top_indices[1]][0] in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"] and idx_to_char[top_indices[0]][0] in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"]):\n","            #       chord1=chord1+[idx_to_char[top_indices[0]]]\n","            #       chord2=chord2+[idx_to_char[top_indices[1]]]\n","            #       prob, _state = run_epoch(session, mtest, test_data, tf.no_op(),beams[0][3])\n","            #       check=True\n","            #       while(check):\n","            #         top_indices2 = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","            #         # print(idx_to_char[top_indices2[0]],idx_to_char[top_indices2[1]])\n","            #         if(idx_to_char[top_indices2[0]]==',' or idx_to_char[top_indices2[1]]==',' or idx_to_char[top_indices2[0]]=='.' or idx_to_char[top_indices2[1]]=='.'):\n","            #           not_accept=1\n","            #         elif(idx_to_char[top_indices2[0]][0] not in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"] and idx_to_char[top_indices2[1]][0] not in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"]):\n","            #           top_indices=top_indices2\n","            #           check=False\n","            #           status=False\n","            #     else:\n","            #       status=False\n","            # else:\n","            if is_sample:\n","                  top_indices = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","            else:\n","                  top_indices = np.argsort(-y1)\n","            b = beams[0]\n","            beam_candidates = []\n","            for i in range(beam_size):\n","                wordix = top_indices[i]\n","                beam_candidates.append((b[0] + y1[wordix], b[1] + [idx_to_char[wordix]], wordix, _state))\n","            beam_candidates.sort(key = lambda x:x[0], reverse = True) # decreasing order\n","            beams = beam_candidates[:beam_size] # truncate to get new beams\n","            print(beams)\n","            print(list_cam_am[1])\n","            #chạy từ 2 vì word_begin+first_word_using_for_create_beam\n","            for xy in range(2,len_of_generation-1,1):\n","                beam_candidates = []\n","                # print(list_cam_am[xy])\n","                if(list_cam_am[xy]=='.'):\n","                  # print('in here')\n","                  for b in beams:\n","                      test_data = np.int32(b[2])\n","                      prob, _state = run_epoch(session, mtest, test_data, tf.no_op(), b[3])\n","                      y1 = np.log(1e-20 + prob.reshape(-1))\n","                      if is_sample:\n","                            top_indices = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","                      else:\n","                          top_indices = np.argsort(-y1)\n","                      if(idx_to_char[top_indices[0]]=='.' or idx_to_char[top_indices[1]]=='.'):\n","                        for i in range(beam_size):\n","                          wordix = top_indices[i]\n","                          beam_candidates.append((b[0] + y1[wordix], b[1] + [idx_to_char[wordix]], wordix, _state))\n","                      else:\n","                        for i in range(beam_size):\n","                          wordix = char_to_idx['.']\n","                          beam_candidates.append((b[0] + y1[wordix], b[1] + [idx_to_char[wordix]], wordix, _state))\n","                elif(list_cam_am[xy]==','):\n","                  for b in beams:\n","                      test_data = np.int32(b[2])\n","                      print('before phay: ',idx_to_char[test_data])\n","                      prob, _state = run_epoch(session, mtest, test_data, tf.no_op(), b[3])\n","                      y1 = np.log(1e-20 + prob.reshape(-1))\n","                      if is_sample:\n","                            top_indices = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","                      else:\n","                          top_indices = np.argsort(-y1)\n","                      if(idx_to_char[top_indices[0]]==',' or idx_to_char[top_indices[1]]==','):\n","                        for i in range(beam_size):\n","                          wordix = top_indices[i]\n","                          beam_candidates.append((b[0] + y1[wordix], b[1] + [idx_to_char[wordix]], wordix, _state))\n","                      else:\n","                        for i in range(beam_size):\n","                          wordix = char_to_idx[',']\n","                          beam_candidates.append((b[0] + y1[wordix], b[1] + [idx_to_char[wordix]], wordix, _state))\n","                else:\n","                  for b in beams:\n","                      test_data = np.int32(b[2])\n","                      prob, _state = run_epoch(session, mtest, test_data, tf.no_op(), b[3])\n","                      y1 = np.log(1e-20 + prob.reshape(-1))\n","                      status=True \n","                      chord1=[]\n","                      chord2=[]\n","                      while(status):\n","                        if is_sample:\n","                            top_indices = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","                        else:\n","                            top_indices = np.argsort(-y1)\n","                        if(idx_to_char[top_indices[0]]=='.' or idx_to_char[top_indices[1]]=='.'):\n","                          not_accept=1\n","                        elif(idx_to_char[top_indices[0]]==',' or idx_to_char[top_indices[1]]==','):\n","                          not_accept=1\n","                        # elif(idx_to_char[top_indices[0]][0] in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"] and idx_to_char[top_indices[1]][0] not in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"]):\n","                        #   chord1=chord1+[idx_to_char[top_indices[0]]]\n","                        #   prob, _state = run_epoch(session, mtest, top_indices[0], tf.no_op(), b[3])\n","                        #   top_indices1 = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","                        #   for top in top_indices1:\n","                        #     if(idx_to_char[top][0] not in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"]):\n","                        #       top_indices[0]=top\n","                        #   # print(top_indices1)\n","                        #   # print(idx_to_char[top_indices[0]],idx_to_char[top_indices[1]])\n","                        # elif(idx_to_char[top_indices[1]][0] in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"] and idx_to_char[top_indices[0]][0] not in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"]):\n","                        #   chord2=chord2+[idx_to_char[top_indices[1]]]\n","                        #   prob, _state = run_epoch(session, mtest, top_indices[1], tf.no_op(), b[3])\n","                        #   top_indices2 = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","                        #   for top in top_indices2:\n","                        #     if(idx_to_char[top][0] not in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"]):\n","                        #       top_indices[1]=top\n","                        #   # print(top_indices2)\n","                        # elif(idx_to_char[top_indices[1]][0] in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"] and idx_to_char[top_indices[0]][0] in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"]):\n","                        #   chord1=chord1+[idx_to_char[top_indices[0]]]\n","                        #   chord2=chord2+[idx_to_char[top_indices[1]]]\n","                        #   prob, _state = run_epoch(session, mtest, test_data, tf.no_op(), b[3])\n","                        #   check=True\n","                        #   while(check):\n","                        #     top_indices2 = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","                        #     # print(idx_to_char[top_indices2[0]],idx_to_char[top_indices2[1]])\n","                        #     if(idx_to_char[top_indices2[0]]==',' or idx_to_char[top_indices2[1]]==',' or idx_to_char[top_indices2[0]]=='.' or idx_to_char[top_indices2[1]]=='.'):\n","                        #       not_accept=1\n","                        #     elif(idx_to_char[top_indices2[0]][0] not in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"] and idx_to_char[top_indices2[1]][0] not in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"]):\n","                        #       top_indices=top_indices2\n","                        #       check=False\n","                        #       status=False\n","                        #   # print(top_indices2)\n","                        else:\n","                          status=False\n","                      # print(idx_to_char[top_indices[0]],idx_to_char[top_indices[1]])\n","                      list_chord=[chord1,chord2]\n","                      # print(list_chord)\n","                      for i in range(beam_size):\n","                          wordix = top_indices[i]\n","                          beam_candidates.append((b[0] + y1[wordix], b[1] + list_chord[i]+[idx_to_char[wordix]], wordix, _state))\n","                beam_candidates.sort(key = lambda x:x[0], reverse = True) # decreasing order\n","                # print(beam_candidates[0][1])\n","                if(list_cam_am[xy]=='.'):\n","                  list_accept_beam=[]\n","                  for beam_check in beam_candidates:\n","                    list_not_chord=module.get_list_not_chord(beam_check[1])\n","                    mot_tu_cuoi=module.get_last_string(list_not_chord,1)\n","                    hai_tu_cuoi=module.get_last_string(list_not_chord,2)\n","                    ba_tu_cuoi=module.get_last_string(list_not_chord,3)\n","                    mot_tu_before_dot=hai_tu_cuoi.split()[0]\n","                    hai_tu_before_dot=ba_tu_cuoi.split()[0]+' '+ba_tu_cuoi.split()[1] \n","                    if(mot_tu_cuoi=='.' and hai_tu_before_dot in list_word_before_dot):\n","                      list_accept_beam.append(beam_check)\n","                    elif(mot_tu_cuoi=='.' and mot_tu_before_dot in list_word_before_dot and hai_tu_before_dot not in list_word_before_dot):\n","                      list_accept_beam.append(beam_check)\n","                  if(len(list_accept_beam)!=4):\n","                    print(list_accept_beam)\n","                  list_accept_beam.sort(key = lambda x:x[0], reverse = True)\n","                  beams=list_accept_beam[:beam_size]\n","                elif(list_cam_am[xy]==','):\n","                  list_accept_beam=[]\n","                  for beam_check in beam_candidates:\n","                    list_not_chord=module.get_list_not_chord(beam_check[1])\n","                    mot_tu_cuoi=module.get_last_string(list_not_chord,1)\n","                    hai_tu_cuoi=module.get_last_string(list_not_chord,2)\n","                    ba_tu_cuoi=module.get_last_string(list_not_chord,3)\n","                    mot_tu_before_phay=hai_tu_cuoi.split()[0]\n","                    hai_tu_before_phay=ba_tu_cuoi.split()[0]+' '+ba_tu_cuoi.split()[1] \n","                    if(mot_tu_cuoi==',' and hai_tu_before_phay in list_word_before_phay):\n","                      list_accept_beam.append(beam_check)\n","                    elif(mot_tu_cuoi==',' and mot_tu_before_phay in list_word_before_phay and hai_tu_before_phay not in list_word_before_phay):\n","                      list_accept_beam.append(beam_check)\n","                  list_accept_beam.sort(key = lambda x:x[0], reverse = True)\n","                  beams=list_accept_beam[:beam_size]\n","                else:\n","                  # hay bo từ khi từ 4 == từ 1 nhỉ?\n","                  list_accept_beam=[]\n","                  for beam_check in beam_candidates:\n","                    list_not_chord=module.get_list_not_chord(beam_check[1])\n","                    mot_tu_cuoi=module.get_last_string(list_not_chord,1)\n","                    hai_tu_cuoi=module.get_last_string(list_not_chord,2)\n","                    # print(hai_tu_cuoi)\n","                    ba_tu_cuoi=module.get_last_string(list_not_chord,3)\n","                    bon_tu_cuoi=module.get_last_string(list_not_chord,4)\n","                    # print(ba_tu_cuoi)\n","                    if(mot_tu_cuoi=='.'):\n","                      kolamgi=1\n","                    elif(hai_tu_cuoi not in list_dong_am and hai_tu_cuoi.split()[0]==hai_tu_cuoi.split()[1] or hai_tu_cuoi=='anh em' or hai_tu_cuoi=='em anh'):\n","                      kolamgi=1\n","                    elif(ba_tu_cuoi in list_ba_tu_lap):\n","                      kolamgi=1\n","                    elif(bon_tu_cuoi in list_bon_tu_lap):\n","                      kolamgi=1\n","                    # test idea xet theo tu cuoi,tu cách cuoi 1 dv\n","                    elif(mot_tu_cuoi==beam_check[1][len(beam_check[1])-3]):\n","                      kolamgi=1\n","                    # elif(mot_tu_cuoi==beam_check[1][len(beam_check[1])-4]):\n","                    #   kolamgi=1\n","                    else:\n","                      list_accept_beam.append(beam_check)\n","                  # if(len(list_accept_beam)!=4):\n","                  #   print(list_accept_beam)\n","                  list_accept_beam.sort(key = lambda x:x[0], reverse = True)\n","                  print(list_accept_beam)\n","                  beams = list_accept_beam[:beam_size] # truncate to get new beams \n","                  # beams = beam_candidates[:beam_size] # cat nhánh\n","                # print(beams)\n","                # print(beams)\n","                list_sentence_lyrics=(word_before_word_begin+' '+' '.join(beams[0][1][0:len(beams[0][1])])).split('.')\n","            if (sys.version_info > (3, 0)):\n","                print(\"Generated Result here2:\\n\")\n","                for yz in range(0,len(list_sentence_cam_am),1):\n","                  print(list_sentence_lyrics[yz])\n","                  print(list_sentence_cam_am[yz])\n","                print('\\n')\n","                print((word_before_word_begin+' '+' '.join(beams[0][1][0:len(beams[0][1])])).replace('.','\\n'))\n","                # with open('Result/'+str(get_number_file_result+1)+'.txt','a',encoding='utf-8') as file1:\n","                #     file1.write((word_before_word_begin+' '+' '.join(beams[0][1][1:len(beams[0][1])])).replace('.','\\n'))\n","            else:\n","                print(\"here\")\n","                print('Generated Result:')\n","                print(join(beams[0][1][1:len(beams[0][1])]))\n","                # with open('Result/'+str(get_number_file_result+1)+'.txt','a',encoding='utf-8') as file1:\n","                #     file1.write((' '.join(beams[0][1][1:len(beams[0][1])])))\n","\n","if __name__ == \"__main__\":\n","    tf.compat.v1.app.run()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Load_model: \n","mi đô đô son sonla son \n"," son son mi đô , la đô la mi mi \n"," rê si la son đô la la son mi rê \n","['mi', 'đô', 'đô', 'son', 'sonla', 'son', '.', 'son', 'son', 'mi', 'đô', ',', 'la', 'đô', 'la', 'mi', 'mi', '.', 'rê', 'si', 'la', 'son', 'đô', 'la', 'la', 'son', 'mi', 'rê', '.']\n","29\n","WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f1b3dabb588>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"],"name":"stdout"},{"output_type":"stream","text":["W1116 15:27:55.874792 139756882962304 rnn_cell_impl.py:698] <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f1b3dabb588>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"],"name":"stderr"},{"output_type":"stream","text":["model loading ...\n","INFO:tensorflow:Restoring parameters from ./model/Model-40\n"],"name":"stdout"},{"output_type":"stream","text":["I1116 15:27:55.965863 139756882962304 saver.py:1284] Restoring parameters from ./model/Model-40\n"],"name":"stderr"},{"output_type":"stream","text":["Done!\n","char:  mưa\n","[(-3.5003292560577393, ['mưa', '2199'], 1561, array([[-0.04414103,  0.00171705,  0.06401332, ..., -0.01165158,\n","         0.0859286 , -0.00929059]], dtype=float32)), (-5.399350643157959, ['mưa', 'mãi'], 4935, array([[-0.04414103,  0.00171705,  0.06401332, ..., -0.01165158,\n","         0.0859286 , -0.00929059]], dtype=float32))]\n","đô\n","[(-5.0763139724731445, ['mưa', '2199', 'rơi'], 3920, array([[ 0.0065187 ,  0.9988618 , -0.01476989, ..., -0.01046828,\n","         0.193157  , -0.24427292]], dtype=float32)), (-7.82442831993103, ['mưa', '2199', 'không'], 2706, array([[ 0.0065187 ,  0.9988618 , -0.01476989, ..., -0.01046828,\n","         0.193157  , -0.24427292]], dtype=float32)), (-9.039462089538574, ['mưa', 'mãi', '1809'], 14481, array([[-1.0059887 ,  0.09603455, -0.8860512 , ...,  0.00351933,\n","        -0.08229209, -0.0287413 ]], dtype=float32)), (-14.929929256439209, ['mưa', 'mãi', 'kia'], 13706, array([[-1.0059887 ,  0.09603455, -0.8860512 , ...,  0.00351933,\n","        -0.08229209, -0.0287413 ]], dtype=float32))]\n","[(-9.87649917602539, ['mưa', '2199', 'không', 'còn'], 6275, array([[-0.27100226,  0.20410988, -0.193155  , ...,  0.01901264,\n","        -0.05029858, -0.46283814]], dtype=float32)), (-10.404764175415039, ['mưa', '2199', 'rơi', '2157'], 10023, array([[-0.4804764 ,  0.01003015, -0.9444345 , ..., -0.03588284,\n","         0.02080123, -0.3567329 ]], dtype=float32)), (-10.830720901489258, ['mưa', '2199', 'rơi', 'không'], 2706, array([[-0.4804764 ,  0.01003015, -0.9444345 , ..., -0.03588284,\n","         0.02080123, -0.3567329 ]], dtype=float32)), (-11.571972131729126, ['mưa', '2199', 'không', 'rơi'], 3920, array([[-0.27100226,  0.20410988, -0.193155  , ...,  0.01901264,\n","        -0.05029858, -0.46283814]], dtype=float32))]\n","[(-14.194520473480225, ['mưa', '2199', 'không', 'còn', 'nhớ'], 14442, array([[-0.00384982,  0.07708289, -0.48657605, ...,  0.00305979,\n","        -0.01300617, -0.35493648]], dtype=float32)), (-21.415678024291992, ['mưa', '2199', 'không', 'còn', '2053'], 13740, array([[-0.00384982,  0.07708289, -0.48657605, ...,  0.00305979,\n","        -0.01300617, -0.35493648]], dtype=float32))]\n","[(-17.03668189048767, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến'], 12478, array([[-9.9032348e-01, -1.4412389e-03, -9.4425595e-01, ...,\n","        -1.5906418e-02,  1.5205196e-05, -3.8942426e-01]], dtype=float32)), (-17.713417530059814, ['mưa', '2199', 'không', 'còn', 'nhớ', '1845'], 8210, array([[-9.9032348e-01, -1.4412389e-03, -9.4425595e-01, ...,\n","        -1.5906418e-02,  1.5205196e-05, -3.8942426e-01]], dtype=float32)), (-28.5747332572937, ['mưa', '2199', 'không', 'còn', '2053', 'giá'], 13960, array([[ 3.1914729e-05,  9.9788874e-01,  2.0206609e-01, ...,\n","        -1.1286853e-02, -3.4468025e-02, -1.7958210e-01]], dtype=float32)), (-30.746254920959473, ['mưa', '2199', 'không', 'còn', '2053', 'cô liêu'], 11955, array([[ 3.1914729e-05,  9.9788874e-01,  2.0206609e-01, ...,\n","        -1.1286853e-02, -3.4468025e-02, -1.7958210e-01]], dtype=float32))]\n","[(-22.160979509353638, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'em'], 7784, array([[ 0.00066076, -0.00665352,  0.00311839, ..., -0.0093836 ,\n","        -0.00110613, -0.13610813]], dtype=float32)), (-22.322425603866577, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những'], 5658, array([[ 0.00066076, -0.00665352,  0.00311839, ..., -0.0093836 ,\n","        -0.00110613, -0.13610813]], dtype=float32)), (-22.81131935119629, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'có'], 11208, array([[ 0.00066076, -0.00665352,  0.00311839, ..., -0.0093836 ,\n","        -0.00110613, -0.13610813]], dtype=float32)), (-23.242342948913574, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'còn'], 6275, array([[ 0.00066076, -0.00665352,  0.00311839, ..., -0.0093836 ,\n","        -0.00110613, -0.13610813]], dtype=float32))]\n","[(-24.874941110610962, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'em', '2157'], 10023, array([[-0.010301  ,  0.01308174,  0.07144356, ..., -0.0161234 ,\n","        -0.041473  , -0.20184979]], dtype=float32)), (-26.295751094818115, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi'], 9955, array([[ 0.00927119,  0.00283785,  0.00552055, ..., -0.00556601,\n","        -0.00400966, -0.23675144]], dtype=float32)), (-26.34118151664734, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'con'], 1212, array([[ 0.00927119,  0.00283785,  0.00552055, ..., -0.00556601,\n","        -0.00400966, -0.23675144]], dtype=float32)), (-26.763813257217407, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'em', 'đi'], 85, array([[-0.010301  ,  0.01308174,  0.07144356, ..., -0.0161234 ,\n","        -0.041473  , -0.20184979]], dtype=float32))]\n","[(-26.747084856033325, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'em', '2157', 'ơi'], 3313, array([[ 2.8560902e-03,  9.9586022e-01,  8.6950429e-02, ...,\n","        -5.2462210e-04, -2.9383507e-01, -2.2084865e-01]], dtype=float32)), (-29.249059200286865, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em'], 7784, array([[ 2.3573544e-04, -3.3730887e-06,  1.3742137e-03, ...,\n","        -2.2026807e-01,  1.5758939e-02, -1.6862205e-01]], dtype=float32)), (-30.267176389694214, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'em', '2157', 'quên'], 11365, array([[ 2.8560902e-03,  9.9586022e-01,  8.6950429e-02, ...,\n","        -5.2462210e-04, -2.9383507e-01, -2.2084865e-01]], dtype=float32)), (-30.46884822845459, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'ngày'], 4252, array([[ 2.3573544e-04, -3.3730887e-06,  1.3742137e-03, ...,\n","        -2.2026807e-01,  1.5758939e-02, -1.6862205e-01]], dtype=float32))]\n","[(-31.37087845802307, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035'], 12568, array([[-0.01326491,  0.00273872,  0.17844701, ...,  0.14489686,\n","        -0.27234823, -0.2082064 ]], dtype=float32)), (-33.204345703125, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '1809'], 14481, array([[-0.01326491,  0.00273872,  0.17844701, ...,  0.14489686,\n","        -0.27234823, -0.2082064 ]], dtype=float32)), (-33.34416604042053, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'em', '2157', 'ơi', 'ngoài'], 2397, array([[-0.03202712,  0.0014666 , -0.01872355, ..., -0.01790977,\n","         0.0002059 , -0.20380786]], dtype=float32))]\n","before phay:  2035\n","before phay:  1809\n","[(-42.96264457702637, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên'], 5468, array([[ 6.1641182e-03, -1.1007913e-01,  1.5783053e-03, ...,\n","        -7.3286980e-02, -8.7996174e-05,  4.9280241e-01]], dtype=float32)), (-43.18332839012146, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'một mình'], 2257, array([[ 6.1641182e-03, -1.1007913e-01,  1.5783053e-03, ...,\n","        -7.3286980e-02, -8.7996174e-05,  4.9280241e-01]], dtype=float32)), (-44.03492522239685, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'ai'], 4191, array([[ 6.1641182e-03, -1.1007913e-01,  1.5783053e-03, ...,\n","        -7.3286980e-02, -8.7996174e-05,  4.9280241e-01]], dtype=float32)), (-45.1133930683136, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'dưới'], 1487, array([[ 6.1641182e-03, -1.1007913e-01,  1.5783053e-03, ...,\n","        -7.3286980e-02, -8.7996174e-05,  4.9280241e-01]], dtype=float32))]\n","[(-44.81725251674652, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh'], 6811, array([[ 6.6401124e-02,  1.2051304e-01, -3.0857513e-02, ...,\n","        -4.8474289e-02, -2.0178215e-06,  7.3596758e-01]], dtype=float32)), (-46.68762731552124, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'một mình', 'bước'], 5755, array([[ 8.8984566e-03, -7.7585900e-01,  4.4737585e-02, ...,\n","        -3.4112610e-02, -4.4996853e-04,  7.0368981e-01]], dtype=float32)), (-48.194971561431885, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'mẹ'], 8417, array([[ 6.6401124e-02,  1.2051304e-01, -3.0857513e-02, ...,\n","        -4.8474289e-02, -2.0178215e-06,  7.3596758e-01]], dtype=float32)), (-48.296140909194946, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'một mình', 'giữa'], 4936, array([[ 8.8984566e-03, -7.7585900e-01,  4.4737585e-02, ...,\n","        -3.4112610e-02, -4.4996853e-04,  7.0368981e-01]], dtype=float32))]\n","[(-48.34372365474701, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau'], 5382, array([[-1.6000070e-02,  2.7153239e-01, -9.8018759e-01, ...,\n","        -2.0739516e-02, -5.3774493e-05,  6.3903093e-01]], dtype=float32)), (-50.22267532348633, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'một mình', 'bước', 'về'], 10338, array([[ 1.1674353e-04,  1.8776573e-01, -9.0567762e-01, ...,\n","        -1.6640097e-02,  3.1539857e-02,  6.9690758e-01]], dtype=float32)), (-50.49032533168793, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'lòng'], 8505, array([[-1.6000070e-02,  2.7153239e-01, -9.8018759e-01, ...,\n","        -2.0739516e-02, -5.3774493e-05,  6.3903093e-01]], dtype=float32)), (-51.88701295852661, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'một mình', 'bước', 'anh'], 6393, array([[ 1.1674353e-04,  1.8776573e-01, -9.0567762e-01, ...,\n","        -1.6640097e-02,  3.1539857e-02,  6.9690758e-01]], dtype=float32))]\n","[(-52.09188711643219, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là'], 306, array([[-0.9234756 , -0.05605461, -0.10344014, ..., -0.05977197,\n","        -0.01127292,  0.3495848 ]], dtype=float32)), (-52.424134612083435, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'như'], 13958, array([[-0.9234756 , -0.05605461, -0.10344014, ..., -0.05977197,\n","        -0.01127292,  0.3495848 ]], dtype=float32)), (-56.99957275390625, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'một mình', 'bước', 'về', 'cơn'], 4127, array([[ 0.01053387, -0.06263827, -0.98379564, ..., -0.05607024,\n","        -0.00240968,  0.6131439 ]], dtype=float32)), (-57.84699869155884, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'một mình', 'bước', 'về', 'câu'], 2294, array([[ 0.01053387, -0.06263827, -0.98379564, ..., -0.05607024,\n","        -0.00240968,  0.6131439 ]], dtype=float32))]\n","[(-55.50515043735504, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày'], 4252, array([[-0.804607  , -0.00302989, -0.05618804, ...,  0.0734008 ,\n","        -0.01061937,  0.34782353]], dtype=float32)), (-57.42268621921539, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', '2200'], 2495, array([[-0.804607  , -0.00302989, -0.05618804, ...,  0.0734008 ,\n","        -0.01061937,  0.34782353]], dtype=float32)), (-57.95435559749603, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'như', 'nụ'], 3616, array([[-0.00770191, -0.0072356 ,  0.10407027, ...,  0.02277229,\n","        -0.00686419,  0.29546806]], dtype=float32)), (-58.70257604122162, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'như', 'câu'], 2294, array([[-0.00770191, -0.0072356 ,  0.10407027, ...,  0.02277229,\n","        -0.00686419,  0.29546806]], dtype=float32))]\n","[(-63.6738144159317, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'từng'], 8333, array([[-1.6427588e-05, -2.5292109e-03,  6.8385847e-04, ...,\n","         2.6467178e-02, -3.5504627e-03,  2.9665375e-01]], dtype=float32)), (-64.24522697925568, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'rồi'], 500, array([[-1.6427588e-05, -2.5292109e-03,  6.8385847e-04, ...,\n","         2.6467178e-02, -3.5504627e-03,  2.9665375e-01]], dtype=float32)), (-64.54608738422394, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'mùa'], 13822, array([[-1.6427588e-05, -2.5292109e-03,  6.8385847e-04, ...,\n","         2.6467178e-02, -3.5504627e-03,  2.9665375e-01]], dtype=float32))]\n","[(-65.21553492546082, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'từng', 'ngày'], 4252, array([[ 9.8563221e-05,  2.8237876e-01,  2.1899387e-01, ...,\n","         1.9207802e-02, -1.8579061e-01,  3.6595249e-01]], dtype=float32)), (-67.88926827907562, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'rồi', '2128'], 10001, array([[ 0.00165833, -0.129015  ,  0.00222492, ..., -0.15654324,\n","        -0.00536395,  0.2508246 ]], dtype=float32)), (-68.34167778491974, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'từng', 'vòng tay'], 12403, array([[ 9.8563221e-05,  2.8237876e-01,  2.1899387e-01, ...,\n","         1.9207802e-02, -1.8579061e-01,  3.6595249e-01]], dtype=float32)), (-68.61753761768341, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'rồi', 'từ'], 5508, array([[ 0.00165833, -0.129015  ,  0.00222492, ..., -0.15654324,\n","        -0.00536395,  0.2508246 ]], dtype=float32))]\n","[(-67.82386183738708, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'từng', 'ngày', '2199'], 1561, array([[ 0.00060776, -0.00025316,  0.00696686, ..., -0.02049152,\n","        -0.00147187,  0.14110771]], dtype=float32)), (-70.4743560552597, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'rồi', '2128', 'khi'], 9955, array([[ 1.2769439e-03,  9.9451983e-01,  4.2356327e-03, ...,\n","        -6.3524954e-02, -1.5508507e-04, -1.9300200e-01]], dtype=float32)), (-73.01517379283905, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'rồi', '2128', 'bao'], 6437, array([[ 1.2769439e-03,  9.9451983e-01,  4.2356327e-03, ...,\n","        -6.3524954e-02, -1.5508507e-04, -1.9300200e-01]], dtype=float32)), (-73.15076947212219, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'từng', 'ngày', 'ngang'], 3584, array([[ 0.00060776, -0.00025316,  0.00696686, ..., -0.02049152,\n","        -0.00147187,  0.14110771]], dtype=float32))]\n","[(-72.6302797794342, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'từng', 'ngày', '2199', 'nhớ'], 14442, array([[-0.00778784,  0.99895656,  0.09534204, ..., -0.03496747,\n","        -0.00152488, -0.31784588]], dtype=float32)), (-72.67322504520416, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'rồi', '2128', 'khi', 'em'], 7784, array([[ 0.00016753,  0.00022631,  0.0292261 , ..., -0.12706067,\n","         0.00490661, -0.15525608]], dtype=float32)), (-74.4305385351181, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'rồi', '2128', 'khi', 'nắng'], 12536, array([[ 0.00016753,  0.00022631,  0.0292261 , ..., -0.12706067,\n","         0.00490661, -0.15525608]], dtype=float32)), (-75.99398970603943, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'từng', 'ngày', '2199', 'ôm'], 4982, array([[-0.00778784,  0.99895656,  0.09534204, ..., -0.03496747,\n","        -0.00152488, -0.31784588]], dtype=float32))]\n","[(-77.43624377250671, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'từng', 'ngày', '2199', 'nhớ', 'bao'], 6437, array([[-0.99093336,  0.04176837, -0.77880603, ..., -0.15620442,\n","        -0.00652163, -0.2035539 ]], dtype=float32)), (-77.611851811409, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'rồi', '2128', 'khi', 'em', 'phải'], 9659, array([[-0.02190821,  0.00242584,  0.3425817 , ..., -0.01123906,\n","        -0.36676803, -0.4499307 ]], dtype=float32)), (-81.7144809961319, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'rồi', '2128', 'khi', 'em', '2116'], 10576, array([[-0.02190821,  0.00242584,  0.3425817 , ..., -0.01123906,\n","        -0.36676803, -0.4499307 ]], dtype=float32)), (-84.22801041603088, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'từng', 'ngày', '2199', 'nhớ', 'tràn'], 7921, array([[-0.99093336,  0.04176837, -0.77880603, ..., -0.15620442,\n","        -0.00652163, -0.2035539 ]], dtype=float32))]\n","[(-80.4508867263794, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'từng', 'ngày', '2199', 'nhớ', 'bao', 'yêu thương'], 1605, array([[ 0.17226645,  0.00046406,  0.01462332, ..., -0.12825647,\n","        -0.0539365 , -0.4383956 ]], dtype=float32)), (-83.38675653934479, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'rồi', '2128', 'khi', 'em', 'phải', 'đứng'], 7006, array([[-9.6116585e-01,  5.4878014e-04,  2.5622004e-01, ...,\n","         2.2871443e-03, -1.6645789e-01, -4.8576990e-01]], dtype=float32)), (-83.72664654254913, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'rồi', '2128', 'khi', 'em', 'phải', 'bơ vơ'], 8683, array([[-9.6116585e-01,  5.4878014e-04,  2.5622004e-01, ...,\n","         2.2871443e-03, -1.6645789e-01, -4.8576990e-01]], dtype=float32))]\n","[(-86.26081562042236, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'từng', 'ngày', '2199', 'nhớ', 'bao', 'yêu thương', 'sẽ'], 3512, array([[-9.8971605e-01,  1.3143624e-05, -4.2684915e-04, ...,\n","         4.4025630e-02,  6.1409470e-02, -2.7434027e-01]], dtype=float32)), (-86.96577227115631, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'rồi', '2128', 'khi', 'em', 'phải', 'đứng', 'đây'], 5498, array([[-1.2442713e-04,  1.9802505e-02,  4.5081076e-01, ...,\n","        -1.7463276e-02, -5.6002695e-02, -5.2922624e-01]], dtype=float32)), (-87.30410957336426, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'từng', 'ngày', '2199', 'nhớ', 'bao', 'yêu thương', '2198'], 547, array([[-9.8971605e-01,  1.3143624e-05, -4.2684915e-04, ...,\n","         4.4025630e-02,  6.1409470e-02, -2.7434027e-01]], dtype=float32)), (-91.70681536197662, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'rồi', '2128', 'khi', 'em', 'phải', 'đứng', 'bơ'], 13574, array([[-1.2442713e-04,  1.9802505e-02,  4.5081076e-01, ...,\n","        -1.7463276e-02, -5.6002695e-02, -5.2922624e-01]], dtype=float32))]\n","[(-89.27803945541382, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'từng', 'ngày', '2199', 'nhớ', 'bao', 'yêu thương', 'sẽ', 'mãi'], 4935, array([[-2.1817824e-02,  1.4414551e-04,  1.7579429e-02, ...,\n","         3.5545644e-01, -1.5771851e-01, -2.5389171e-01]], dtype=float32)), (-90.1310305595398, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'từng', 'ngày', '2199', 'nhớ', 'bao', 'yêu thương', 'sẽ', '2035'], 12568, array([[-2.1817824e-02,  1.4414551e-04,  1.7579429e-02, ...,\n","         3.5545644e-01, -1.5771851e-01, -2.5389171e-01]], dtype=float32)), (-91.0199567079544, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'rồi', '2128', 'khi', 'em', 'phải', 'đứng', 'đây', 'và'], 12535, array([[-8.1200343e-01, -3.3842563e-04,  2.6052976e-02, ...,\n","        -1.4784592e-01, -1.8533586e-02, -3.5192144e-01]], dtype=float32)), (-91.65227043628693, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'rồi', '2128', 'khi', 'em', 'phải', 'đứng', 'đây', '1845'], 8210, array([[-8.1200343e-01, -3.3842563e-04,  2.6052976e-02, ...,\n","        -1.4784592e-01, -1.8533586e-02, -3.5192144e-01]], dtype=float32))]\n","[(-91.64128255844116, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'từng', 'ngày', '2199', 'nhớ', 'bao', 'yêu thương', 'sẽ', 'mãi', 'không'], 2706, array([[-1.0012393 ,  0.09626754, -0.90628743, ...,  0.04817704,\n","        -0.00743506, -0.3090165 ]], dtype=float32)), (-93.5212094783783, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'từng', 'ngày', '2199', 'nhớ', 'bao', 'yêu thương', 'sẽ', '2035', 'trao'], 12193, array([[ 0.01474362,  0.9951237 , -0.27865443, ..., -0.00265896,\n","        -0.11079542,  0.28220585]], dtype=float32)), (-94.60802984237671, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'từng', 'ngày', '2199', 'nhớ', 'bao', 'yêu thương', 'sẽ', 'mãi', 'cùng'], 2806, array([[-1.0012393 ,  0.09626754, -0.90628743, ...,  0.04817704,\n","        -0.00743506, -0.3090165 ]], dtype=float32)), (-97.54621744155884, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'từng', 'ngày', '2199', 'nhớ', 'bao', 'yêu thương', 'sẽ', '2035', 'cuốn'], 2482, array([[ 0.01474362,  0.9951237 , -0.27865443, ..., -0.00265896,\n","        -0.11079542,  0.28220585]], dtype=float32))]\n","[(-93.92961502075195, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'từng', 'ngày', '2199', 'nhớ', 'bao', 'yêu thương', 'sẽ', 'mãi', 'không', 'bao giờ'], 14497, array([[-0.3779618 ,  0.005605  , -0.22805813, ...,  0.12433176,\n","        -0.01941695, -0.32944658]], dtype=float32)), (-96.5240228176117, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'từng', 'ngày', '2199', 'nhớ', 'bao', 'yêu thương', 'sẽ', '2035', 'trao', 'cho'], 10258, array([[-9.9810606e-01,  4.5025018e-03, -5.0452477e-01, ...,\n","        -4.0060237e-02, -8.1992075e-05,  2.3364772e-01]], dtype=float32)), (-97.09964990615845, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'từng', 'ngày', '2199', 'nhớ', 'bao', 'yêu thương', 'sẽ', 'mãi', 'không', '1809'], 14481, array([[-0.3779618 ,  0.005605  , -0.22805813, ...,  0.12433176,\n","        -0.01941695, -0.32944658]], dtype=float32)), (-98.80655598640442, ['mưa', '2199', 'không', 'còn', 'nhớ', 'đến', '.', 'những', 'khi', 'em', '2035', ',', 'bên', 'cạnh', 'nhau', 'là', 'ngày', '.', 'từng', 'ngày', '2199', 'nhớ', 'bao', 'yêu thương', 'sẽ', '2035', 'trao', 'người'], 7063, array([[-9.9810606e-01,  4.5025018e-03, -5.0452477e-01, ...,\n","        -4.0060237e-02, -8.1992075e-05,  2.3364772e-01]], dtype=float32))]\n","Generated Result here2:\n","\n"," mưa 2199 không còn nhớ đến \n","mi đô đô son sonla son \n"," những khi em 2035 , bên cạnh nhau là ngày \n"," son son mi đô , la đô la mi mi \n"," từng ngày 2199 nhớ bao yêu thương sẽ mãi không bao giờ\n"," rê si la son đô la la son mi rê \n","\n","\n"," mưa 2199 không còn nhớ đến \n"," những khi em 2035 , bên cạnh nhau là ngày \n"," từng ngày 2199 nhớ bao yêu thương sẽ mãi không bao giờ\n"],"name":"stdout"},{"output_type":"error","ename":"SystemExit","evalue":"ignored","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\n"]},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"gC5hz1f_wRtN"},"source":["#coding:utf-8\n","import tensorflow as tf\n","import sys,time\n","import numpy as np\n","import pickle as cPickle\n","import os\n","import random\n","import Config\n","import Model\n","import codecs\n","from random import choice\n","config_tf = tf.compat.v1.ConfigProto()\n","config_tf.gpu_options.allow_growth = True\n","config_tf.inter_op_parallelism_threads = 1\n","config_tf.intra_op_parallelism_threads = 1\n","\n","config = Config.Config()\n","\n","char_to_idx, idx_to_char = cPickle.load(open(config.model_path+'.voc', 'rb'))\n","\n","config.vocab_size = len(char_to_idx)\n","is_sample = config.is_sample\n","is_beams = config.is_beams\n","beam_size = config.beam_size\n","# len_of_generation = config.len_of_generation\n","start_sentence = \"anh\"\n","if (len(sys.argv) == 2):\n","    if (sys.version_info > (3, 0)):\n","        start_sentence = sys.argv[1]\n","    else:\n","        start_sentence = sys.argv[1].decode(\"utf-8\")\n","#doc bang cam am\n","filename='Result_cam_am/mi_100_True.txt'\n","with open(filename, encoding='utf-8') as f:\n","        data = f.read()\n","# data=data.split('\\n')[0]+'\\n'+data.split('\\n')[1]\n","print(data)\n","list_cam_am=(data.replace('\\n','.')+' .').split()\n","len_list_cam_am=len(list_cam_am)\n","print(list_cam_am)\n","print(len_list_cam_am)\n","####khoi tao cac danh sach xu ly cau và lap tu\n","list_dong_am=['mãi mãi','luôn luôn']\n","list_ba_tu_lap=['em yêu em','anh yêu anh','yêu em yêu','yêu anh yêu','em ơi em','anh ơi anh']\n","list_bon_tu_lap=[\"anh vẫn yêu anh\",'em vẫn yêu em','anh vẫn mong anh','em vẫn mong em','em đã yêu em',\n","                  \"anh rất yêu anh\",\"em rất yêu em\",'anh đã yêu anh','em thật nhiều em ','em sẽ yêu em',\n","                  'anh sẽ yêu anh']\n","def run_epoch(session, m, data, eval_op, state=None):\n","    \"\"\"Runs the model on the given data.\"\"\"\n","    x = data.reshape((1,1))\n","    prob, _state, _ = session.run([m._prob, m.final_state, eval_op],\n","                         {m.input_data: x,\n","                          m.initial_state: state})\n","    return prob, _state\n","\n","def main(_):\n","    with tf.Graph().as_default(), tf.compat.v1.Session(config=config_tf) as session:\n","        config.batch_size = 1\n","        config.num_steps = 1\n","\n","        initializer = tf.random_uniform_initializer(-config.init_scale,\n","                                                config.init_scale)\n","        with tf.compat.v1.variable_scope(\"model\", reuse=None, initializer=initializer):\n","            mtest = Model.Model(is_training=False, config=config)\n","        model_saver = tf.compat.v1.train.Saver()\n","        print(\"model loading ...\")\n","        model_saver.restore(session, config.model_path+'-%d'%config.save_time)\n","        print (\"Done!\")\n","        char_list = (start_sentence.split());\n","        for word in char_list:\n","            len_of_generation=len_list_cam_am\n","            if not is_beams:\n","                start_idx = char_to_idx[word]\n","                _state = mtest.initial_state.eval()\n","                test_data = np.int32([start_idx])\n","                prob, _state = run_epoch(session, mtest, test_data, tf.no_op(), _state)\n","                gen_res = []\n","                char = word\n","                try:\n","                    char_index = char_to_idx[char]\n","                except KeyError:\n","                    char_index = np.argmax(prob.reshape(-1))\n","                prob, _state = run_epoch(session, mtest, np.int32([char_index]), tf.no_op(), _state)\n","                gen_res.append(char)\n","                if is_sample:\n","                    gen = np.random.choice(config.vocab_size, 1, p=prob.reshape(-1))\n","                    gen = gen[0]\n","                else:\n","                    gen = np.argmax(prob.reshape(-1))\n","                test_data = np.int32(gen)\n","                gen_res.append(idx_to_char[gen])\n","                print(gen_res)\n","                for i in range(len_of_generation-1):\n","                    prob, _state = run_epoch(session, mtest, test_data, tf.no_op(), _state)\n","                    if is_sample:\n","                        gen = np.random.choice(config.vocab_size, 1, p=prob.reshape(-1))\n","                        gen = gen[0]\n","                    else:\n","                        gen = np.argmax(prob.reshape(-1))\n","                    test_data = np.int32(gen)\n","                    gen_res.append(idx_to_char[gen])\n","                print(\"Generated Result:\\n {0}\".format((' ').join(gen_res).replace('.','\\n')))\n","            else:\n","              #dua word_begin vao tao beam\n","                start_idx = char_to_idx[word]\n","                _state = mtest.initial_state.eval()\n","                beams = [(0.0, [idx_to_char[start_idx]], idx_to_char[start_idx])]\n","                test_data = np.int32([start_idx])\n","                prob, _state = run_epoch(session, mtest, test_data, tf.no_op(), _state)\n","                y1 = np.log(1e-20 + prob.reshape(-1))\n","                beams = [(beams[0][0], beams[0][1], beams[0][2], _state)]\n","                char = word\n","                try:\n","                    char_index = char_to_idx[char]\n","                except KeyError:\n","                    top_indices = np.argsort(-y1)\n","                    char_index = top_indices[0]\n","                prob, _state = run_epoch(session, mtest, np.int32([char_index]), tf.no_op(), beams[0][3])\n","                y1 = np.log(1e-20 + prob.reshape(-1))\n","                beams = [(beams[0][0], beams[0][1], char_index, _state)]\n","                # gen text\n","                if is_sample:\n","                    top_indices = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","                else:\n","                    top_indices = np.argsort(-y1)\n","                b = beams[0]\n","                beam_candidates = []\n","                for i in range(beam_size):\n","                    wordix = top_indices[i]\n","                    beam_candidates.append((b[0] + y1[wordix], b[1] + [idx_to_char[wordix]], wordix, _state))\n","                beam_candidates.sort(key = lambda x:x[0], reverse = True) # decreasing order\n","                beams = beam_candidates[:beam_size] # truncate to get new beams\n","                print(beams)\n","                print(len_of_generation-1)\n","                for index in range(1,len_of_generation-1,1):\n","                    # print(list_cam_am[index])\n","                    beam_candidates = []\n","                    test_data = np.int32(b[2])\n","                    prob, _state = run_epoch(session, mtest, test_data, tf.no_op(), b[3])\n","                    y1 = np.log(1e-20 + prob.reshape(-1))\n","                    for b in beams:\n","                      if(list_cam_am[index]==','):\n","                        status=True\n","                        while(status):\n","                          if is_sample:\n","                            top_indices = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","                          else:\n","                            top_indices = np.argsort(-y1)\n","                          word1=idx_to_char[top_indices[0]]\n","                          word2=idx_to_char[top_indices[1]]\n","                          if(',' in [word1,word2]):\n","                            status=False\n","                          for i in range(beam_size):\n","                              wordix = top_indices[i]\n","                              beam_candidates.append((b[0] + y1[wordix], b[1] + [idx_to_char[wordix]], wordix, _state))\n","                      elif(list_cam_am[index]=='.'):\n","                        status=True\n","                        while(status):\n","                          if is_sample:\n","                              top_indices = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","                          else:\n","                              top_indices = np.argsort(-y1)\n","                          word1=idx_to_char[top_indices[0]]\n","                          word2=idx_to_char[top_indices[1]]\n","                          if('.' in [word1,word2]):\n","                            status=False\n","                          for i in range(beam_size):\n","                              wordix = top_indices[i]\n","                              beam_candidates.append((b[0] + y1[wordix], b[1] + [idx_to_char[wordix]], wordix, _state))\n","                      else:\n","                        status=True\n","                        while(status):\n","                          if is_sample:\n","                                top_indices = np.random.choice(config.vocab_size, beam_size, replace=False, p=prob.reshape(-1))\n","                          else:\n","                              top_indices = np.argsort(-y1)\n","                          word1=idx_to_char[top_indices[0]]\n","                          word2=idx_to_char[top_indices[1]]\n","                          # print(word1,word2)\n","                          if(word1=='.' or word2=='.' or word1==',' or word2==','):\n","                            not_process=1\n","                          else:\n","                            status=False\n","                        for i in range(beam_size):\n","                            wordix = top_indices[i]\n","                            beam_candidates.append((b[0] + y1[wordix], b[1] + [idx_to_char[wordix]], wordix, _state))\n","                    beam_candidates.sort(key = lambda x:x[0], reverse = True) # decreasing order\n","                    print(list_cam_am[index])\n","                    if(list_cam_am[index]=='.'):\n","                      list_accept_beam=[]\n","                      # print(beam_candidates)\n","                      for beam_check in beam_candidates:\n","                        phan_tu_cuoi=beam_check[1][len(beam_check[1])-1]\n","                        if(phan_tu_cuoi=='.'):\n","                          list_accept_beam.append(beam_check)\n","                      beams=list_accept_beam[:beam_size]\n","                      print(beams)\n","                    elif(list_cam_am[index]==','):\n","                      list_accept_beam=[]\n","                      for beam_check in beam_candidates:\n","                        phan_tu_cuoi=beam_check[1][len(beam_check[1])-1]\n","                        if(phan_tu_cuoi==','):\n","                          list_accept_beam.append(beam_check)\n","                      beams=list_accept_beam[:beam_size]\n","                      print(beams)\n","                    else:\n","                        # xử lý lặp từ với 2 từ gần nhau:\n","                        list_dong_am=['mãi mãi','luôn luôn']\n","                        list_ba_tu_lap=['em yêu em','anh yêu anh','yêu em yêu','yêu anh yêu','em ơi em','anh ơi anh']\n","                        list_bon_tu_lap=[\"anh vẫn yêu anh\",'em vẫn yêu em','anh vẫn mong anh','em vẫn mong em','em đã yêu em',\n","                                        \"anh rất yêu anh\",\"em rất yêu em\",'anh đã yêu anh','em thật nhiều em ','em sẽ yêu em',\n","                                        'anh sẽ yêu anh']\n","                        # hay bo từ khi từ 4 == từ 1 nhỉ?\n","                        list_accept_beam=[]\n","                        for beam_check in beam_candidates:\n","                          mot_tu_cuoi=beam_check[1][len(beam_check[1])-1]\n","                          hai_tu_cuoi=beam_check[1][len(beam_check[1])-2]+ ' ' +beam_check[1][len(beam_check[1])-1]\n","                          ba_tu_cuoi=beam_check[1][len(beam_check[1])-3]+ ' ' +beam_check[1][len(beam_check[1])-2]+ ' ' +beam_check[1][len(beam_check[1])-1]\n","                          bon_tu_cuoi=beam_check[1][len(beam_check[1])-4]+' '+beam_check[1][len(beam_check[1])-3]+ ' ' +beam_check[1][len(beam_check[1])-2]+ ' ' +beam_check[1][len(beam_check[1])-1]\n","                          # print(ba_tu_cuoi)\n","                          if(mot_tu_cuoi=='.'):\n","                            kolamgi=1\n","                          elif(hai_tu_cuoi not in list_dong_am and beam_check[1][len(beam_check[1])-2]==beam_check[1][len(beam_check[1])-1]):\n","                            kolamgi=1\n","                          elif(ba_tu_cuoi in list_ba_tu_lap):\n","                            kolamgi=1\n","                          elif(bon_tu_cuoi in list_bon_tu_lap):\n","                            kolamgi=1\n","                          # test idea xet theo tu cuoi,tu cách cuoi 1 dv\n","                          elif(mot_tu_cuoi==beam_check[1][len(beam_check[1])-3]):\n","                            kolamgi=1\n","                          # elif(mot_tu_cuoi==beam_check[1][len(beam_check[1])-4]):\n","                          #   kolamgi=1\n","                          else:\n","                            list_accept_beam.append(beam_check)\n","                    # print(list_1_tu_cuoi)\n","                    beams = beam_candidates[:beam_size] # truncate to get new beams\n","                result=' '.join(beams[0][1][0:len(beams[0][1])])\n","                print(len(result.split()))\n","                if (sys.version_info > (3, 0)):\n","                    print(\"Generated Result:\\n {0}\".format(' '.join(beams[0][1][0:len(beams[0][1])]).replace('.','\\n')))\n","                else:\n","                    print('Generated Result:')\n","                    print(' '.join(' '.join(beams[0][1][1:len(beams[0][1])])))\n","\n","\n","if __name__ == \"__main__\":\n","    tf.compat.v1.app.run()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zvNpKUMdOHwO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606287043494,"user_tz":-420,"elapsed":8735,"user":{"displayName":"Tuấn Tô Huỳnh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXJTUPH4_1EZB9BBjAzQVNJqit9A6jglPVLllHoQ=s64","userId":"04616076029360035217"}},"outputId":"731f2354-240f-4460-d4f3-f8937c3bb3c4"},"source":["!pip install underthesea"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Collecting underthesea\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/22/1f5a4e4ae4dc318455cbbf2a154d557b28cc56437adce75583d2e22c38c3/underthesea-1.2.2-py3-none-any.whl (7.5MB)\n","\u001b[K     |████████████████████████████████| 7.5MB 7.5MB/s \n","\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from underthesea) (0.17.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from underthesea) (4.41.1)\n","Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.6/dist-packages (from underthesea) (7.1.2)\n","Collecting scikit-learn<0.22,>=0.20\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/c5/d2238762d780dde84a20b8c761f563fe882b88c5a5fb03c056547c442a19/scikit_learn-0.21.3-cp36-cp36m-manylinux1_x86_64.whl (6.7MB)\n","\u001b[K     |████████████████████████████████| 6.7MB 44.7MB/s \n","\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from underthesea) (0.8.7)\n","Collecting unidecode\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n","\u001b[K     |████████████████████████████████| 245kB 52.5MB/s \n","\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from underthesea) (3.2.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from underthesea) (2.23.0)\n","Collecting python-crfsuite>=0.9.6\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/99/869dde6dbf3e0d07a013c8eebfb0a3d30776334e0097f8432b631a9a3a19/python_crfsuite-0.9.7-cp36-cp36m-manylinux1_x86_64.whl (743kB)\n","\u001b[K     |████████████████████████████████| 747kB 53.0MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn<0.22,>=0.20->underthesea) (1.18.5)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn<0.22,>=0.20->underthesea) (1.4.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->underthesea) (1.15.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->underthesea) (2020.11.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->underthesea) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->underthesea) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->underthesea) (3.0.4)\n","Installing collected packages: scikit-learn, unidecode, python-crfsuite, underthesea\n","  Found existing installation: scikit-learn 0.22.2.post1\n","    Uninstalling scikit-learn-0.22.2.post1:\n","      Successfully uninstalled scikit-learn-0.22.2.post1\n","Successfully installed python-crfsuite-0.9.7 scikit-learn-0.21.3 underthesea-1.2.2 unidecode-1.1.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IaTfd3ZgOMg8"},"source":["#train_new\n","#coding:utf-8\n","import tensorflow as tf\n","import sys,time\n","import numpy as np\n","import pickle as cPickle\n","import os\n","import Config\n","import Model\n","import process_data_intchord\n","import datetime\n","import json\n","# import convert_string_to_int_chord\n","if not os.path.exists('./model'):\n","    os.makedirs('./model')\n","\n","# config_tf = tf.ConfigProto()\n","config_tf = tf.compat.v1.ConfigProto()\n","config_tf.gpu_options.allow_growth = True\n","config_tf.inter_op_parallelism_threads = 1\n","config_tf.intra_op_parallelism_threads = 1\n","\n","# file = \"lyrics/all_training_lyrics.txt\"\n","# if (sys.version_info > (3, 0)):\n","#     data = open(file, encoding=\"utf-8\").read()\n","# else:\n","#     data = open(file,'r').read()\n","#     data = data.decode('utf-8')\n","# with open(file, encoding='utf-8') as f:\n","#     data = f.read()\n","# get list and save in txt file\n","data=process_data_intchord.get_list_word()\n","# with open('list_word.txt','a',encoding='utf-8') as file1:\n","#     file1.write(str(data))\n","# with open('list_word.txt', encoding='utf-8') as f:\n","#     data = f.read()\n","chars = list(set(data)) #char vocabulary\n","# print(chars)\n","data_size, _vocab_size = len(data), len(chars)\n","# print(data_size)\n","# print(_vocab_size)\n","# def load_file_json(file):\n","#     with open(file,'r',encoding='utf-8') as file_output:\n","#         data_word1 = json.load(file_output)\n","#     return data_word1\n","print (\"data has {0} characters, {1} unique.\".format(data_size, _vocab_size))\n","char_to_idx = { ch:i for i,ch in enumerate(chars) }\n","idx_to_char = { i:ch for i,ch in enumerate(chars) }\n","# char_to_idx = load_file_json('word_to_index.json')\n","# idx_to_char =load_file_json('index_to_word.json')\n","# print(char_to_idx)\n","# write_char_to_idx=json.dump(char_to_idx)\n","# write_idx_to_char=json.dump(idx_to_char)\n","# with open('word_to_index.json', 'w+',encoding='utf-8') as outfile:\n","#     json.dump(char_to_idx,outfile,ensure_ascii=False)\n","\n","# with open('index_to_word.json', 'w+',encoding='utf-8') as outfile2:\n","#     json.dump(idx_to_char,outfile2,ensure_ascii=False)\n","# with open('./word_to_index_word.txt', 'w', encoding='utf8') as w:\n","config = Config.Config()\n","config.vocab_size = _vocab_size\n","\n","cPickle.dump((char_to_idx, idx_to_char), open(config.model_path+'.voc','wb'), protocol=cPickle.HIGHEST_PROTOCOL)\n","\n","context_of_idx = [char_to_idx[ch] for ch in data]\n","\n","def data_iterator(raw_data, batch_size, num_steps):\n","    raw_data = np.array(raw_data, dtype=np.int32)\n","\n","    data_len = len(raw_data)\n","    batch_len = data_len // batch_size\n","    data = np.zeros([batch_size, batch_len], dtype=np.int32)\n","    for i in range(batch_size):\n","        data[i] = raw_data[batch_len * i:batch_len * (i + 1)] # data 的 shape 是 (batch_size, batch_len)，每一行是連貫的一段，一次可輸入多個段落\n","\n","    epoch_size = (batch_len - 1) // num_steps\n","\n","    if epoch_size == 0:\n","        raise ValueError(\"epoch_size == 0, decrease batch_size or num_steps\")\n","\n","    for i in range(epoch_size):\n","        x = data[:, i*num_steps:(i+1)*num_steps]\n","        y = data[:, i*num_steps+1:(i+1)*num_steps+1] # y 就是 x 的錯一位，即下一個詞\n","        yield (x, y)\n","\n","def run_epoch(session, m, data, eval_op):\n","    \"\"\"Runs the model on the given data.\"\"\"\n","    epoch_size = ((len(data) // m.batch_size) - 1) // m.num_steps\n","    start_time = time.time()\n","    costs = 0.0\n","    iters = 0\n","    state = m.initial_state.eval()\n","    for step, (x, y) in enumerate(data_iterator(data, m.batch_size,\n","                                                    m.num_steps)):\n","        cost, state, _ = session.run([m.cost, m.final_state, eval_op], # x 和 y 的 shape 都是 (batch_size, num_steps)\n","                                 {m.input_data: x,\n","                                  m.targets: y,\n","                                  m.initial_state: state})\n","        costs += cost\n","        iters += m.num_steps\n","\n","        if step and step % (epoch_size // 10) == 0:\n","            print(\"%.2f perplexity: %.3f cost-time: %.2f s\" %\n","                (step * 1.0 / epoch_size, np.exp(costs / iters),\n","                 (time.time() - start_time)))\n","            start_time = time.time()\n","\n","    return np.exp(costs / iters)\n","\n","def main(_):\n","    train_data = context_of_idx\n","    # with tf.Graph().as_default(), tf.Session(config=config_tf) as session:\n","    with tf.Graph().as_default(), tf.compat.v1.Session(config=config_tf) as session:\n","        initializer = tf.random_uniform_initializer(-config.init_scale,\n","                                                config.init_scale)\n","        # with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n","        with tf.compat.v1.variable_scope(\"model\", reuse=None, initializer=initializer):\n","            m = Model.Model(is_training=True, config=config)\n","\n","        tf.compat.v1.global_variables_initializer().run()\n","\n","        # model_saver = tf.train.Saver(tf.global_variables())\n","        model_saver=tf.compat.v1.train.Saver(tf.compat.v1.global_variables())\n","\n","        for i in range(config.iteration):\n","            print(\"Training Epoch: %d ...\" % (i+1))\n","            train_perplexity = run_epoch(session, m, train_data, m.train_op)\n","            print(\"Epoch: %d Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n","\n","            if (i+1) % config.save_freq == 0:\n","                print (\"model saving ...\")\n","                model_saver.save(session, config.model_path+'-%d'%(i+1))\n","                print (\"Done!\")\n","\n","if __name__ == \"__main__\":\n","    # tf.app.run()\n","    tf.compat.v1.app.run()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5mxJQ549odoU"},"source":["!pip install underthesea"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zw0kDCKenxmF","executionInfo":{"status":"ok","timestamp":1606211061734,"user_tz":-420,"elapsed":4161,"user":{"displayName":"Tuấn Tô Huỳnh","photoUrl":"","userId":"00317117433846939709"}},"outputId":"d8b94d06-5d19-423a-b74d-68ced55f0b49"},"source":["#test train\n","# !pip install underthesea\n","!cd /content/drive/\n","!pwd\n","!cd ../../tom_chang\n","!ls\n","!python /content/drive/MyDrive/tom_chang/train.py"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/.shortcut-targets-by-id/1c92CkMFaSZZhn7nwqVSAkNfoZLxiHnC1/tom_chang_new\n","/bin/bash: line 0: cd: ../../tom_chang: No such file or directory\n","before_dot.csv\t   lethihongnhien.py\t     Model.pyc\n","before_phay.csv    lyric_generator_part1.py  Model_test.py\n","cam_am\t\t   lyric_generator_part2.py  module.py\n","cam_am.py\t   Lyrics_generation.ipynb   process_data_intchord.py\n","Config.py\t   model\t\t     __pycache__\n","data_final2_1.csv  Model_bk.py\t\t     report.txt\n","file_config\t   Model.py\t\t     Result_cam_am\n","2020-11-24 16:44:18.339284: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2020-11-24 16:44:18.339319: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/tom_chang/train.py\", line 29, in <module>\n","    data=convert_string_to_int_chord.get_list_word()\n","  File \"/content/drive/MyDrive/tom_chang/convert_string_to_int_chord.py\", line 20, in get_list_word\n","    with open(type_lyric+'.csv','r',encoding='utf-8') as f:\n","FileNotFoundError: [Errno 2] No such file or directory: 'final.csv'\n"],"name":"stdout"}]}]}